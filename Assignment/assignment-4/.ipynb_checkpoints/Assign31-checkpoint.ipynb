{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment #3 - Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\"> Anirudh Narayanan </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Overview (Objective and Approach)\n",
    "\n",
    "The key idea of this assignment is to achieve non-linearity in classifying/fitting the training data. There are many ways of achieving non-linearity. The first thing to keep in mind while doing so , that higher order polynomials need to be leveraged. The problem with performing trial and error with higher order polynomials, polynomial regression, is that the number of possibilities are unfathomable, and trying all of such could be very computationally expensive. To avoid this problem, we set our focus on neural networks, which periodically perform separate tasks ( each task in a separate layer), and the result of this, when we do find the right polynomials and the right elements to perform the right classification /regression ( without overfitting)\n",
    "\n",
    "It can be proved that a neural net with an activation function (relu, tanh, sigmoid) can naturally very easily fit any given training data without fail. It can fit any possible curve by this process. Eventually, we notice that overdoing this will lead us to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " \n",
    "#  Data Description\n",
    "​\n",
    "The data represents different weather features in Australia, like weather, temperature, humidity, windspeed etc. This data can be very useful for regression in that, the past can be used to predict how the future weather conditions can be. This data can also be very useful when considering classification, in that places/years can be classified upon, based on the data which they have. A good example is Melbourne Aiport vs Portland's Humidity vs Rainfall which has clear demarcations, and can be used during classifying new data during one of these years, or of future data which is of Rainfall/Humidity during any part of the year.\n",
    "​\n",
    "# Column Pre-Processing\n",
    "​\n",
    "For this process, each of the columns were evaluated with respect to their null values. The reason for this was, removing None values by the row causes issues, in that there may be columns which don't have enough data in them, and hence might have a large percentage of them as None. Due to this, the percentage of null values in each column were evaluated before processing. If the percentage of the column's null values were more than 70%, the entire columns were dropped in lieu of insufficient information.\n",
    "​\n",
    "# Row Pre-Processing\n",
    "​\n",
    "Row Pre-Processing¶\n",
    "Further, the rows were removed by iteration through the entire data, thereby too many rows were not removed just because some columns had bad information. This had to be done, using a dual loop. Each column had to be iterated for each row iteration. This was because, pandas hashes the information column wise, and not row/dual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Data\n",
    "\n",
    "Introduce your data and visualize them. Describe your observations about the data.\n",
    "You can reuse the data that you examined in Assignment #1 (of course for regression). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLOTS FOR UNDERSTANDING OR ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mpl_toolkits import mplot3d\n",
    "import copy\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"ausweather_preprocessed.csv\",sep=\"\\t\")\n",
    "\n",
    "grouped_by_month_rainfall = df.groupby([df.Date.str[:7],\"Location\"])[\"Location\",\"MaxTemp\",\"Humidity9am\"].mean().reset_index()\n",
    "\n",
    "#print grouped_by_month_rainfall\n",
    "grouped_2013 = grouped_by_month_rainfall\n",
    "grouped_2013 = grouped_by_month_rainfall[ (grouped_2013[\"Location\"]==\"Katherine\")  | (grouped_2013[\"Location\"]==\"Bendigo\") ]\n",
    "\n",
    "#print grouped_2013\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.title(\"Humidity vs Max Temperature of 2 Australian Locations\")\n",
    "plt.xlabel('Humidity', fontsize=18)\n",
    "plt.ylabel('Max Temperature', fontsize=16)\n",
    "#EXAMPLE OF A SIMPLE TWO DIVISION CLUSTER. DATA FROM EITHER ONE OF THE SOURCES CAN BE CLASSIFIED TO EITHER ONE BASED ON SOME CLASSIFICATION ALGORITHM\n",
    "for name,group in grouped_2013.groupby(\"Location\"):\n",
    "    plt.scatter(group[\"Humidity9am\"],group[\"MaxTemp\"],label=name)\n",
    "    plt.legend()\n",
    " \n",
    "\n",
    "\n",
    "grouped3d = df.groupby([df.Date.str[:12],\"Location\"])[\"Location\",\"MaxTemp\",\"Humidity9am\",\"Rainfall\"].mean().reset_index()\n",
    "\n",
    "groupwithout = copy.deepcopy(grouped3d)\n",
    "grouped3d = grouped3d[grouped3d[\"Location\"]==\"Canberra\"]\n",
    "\n",
    "\n",
    "\n",
    "#print grouped3d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA HERE CAN BE CLASSIFIED BETWEEN 2008 and 2010\n",
    "\n",
    "#print grouped3d\n",
    "plt.figure(figsize=(15,8))\n",
    "fig = plt.figure()\n",
    "fig.set_figheight(15)\n",
    "fig.set_figwidth(15)\n",
    "plt.figure(figsize=(15,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.set_title(\"Max Temperature, Humidity, Rainfall of 2008 vs 2010 (Classifiable)\")\n",
    "ax.set_xlabel('Max Temperature', fontsize=18)\n",
    "ax.set_ylabel('Humidity 9 am', fontsize=16)\n",
    "ax.set_zlabel('Rainfall', fontsize=16)\n",
    "#for name,group in grouped3d.groupby(grouped3d.Date.str[:4]):\n",
    "ax.scatter(groupwithout[\"MaxTemp\"],groupwithout[\"Humidity9am\"],groupwithout[\"Rainfall\"])\n",
    "#ax.plot(X[:,1],answers)\n",
    "     #ax.legend()\n",
    "\n",
    "\n",
    "canberra_rainfall_df = df.groupby([df.Date.str[:7],\"Location\"]).mean().reset_index()[[\"Humidity3pm\",\"Rainfall\",\"Location\"]]\n",
    "canberra_df_humidity = df.groupby([df.Date.str[:7]]).mean().reset_index()[[\"Date\",\"Humidity3pm\"]]\n",
    "canberra_df_clouds = df.groupby([df.Date.str[:7]]).mean().reset_index()[[\"Date\",\"Cloud3pm\"]]\n",
    "\n",
    "#print canberra_rainfall_df[(canberra_rainfall_df[\"Location\"]==\"Katherine\")  | (canberra_rainfall_df[\"Location\"]==\"Bendigo\")]\n",
    "#print canberra_rainfall_df[(canberra_rainfall_df[\"Location\"]==\"Portland\")  | (canberra_rainfall_df[\"Location\"]==\"MelbourneAirport\")]\n",
    "#canberra_rainfall_df = canberra_rainfall_df[(canberra_rainfall_df[\"Location\"]==\"Katherine\")  | (canberra_rainfall_df[\"Location\"]==\"Bendigo\")]\n",
    "\n",
    "canberra_rainfall_df = canberra_rainfall_df[(canberra_rainfall_df[\"Location\"]==\"Portland\")  | (canberra_rainfall_df[\"Location\"]==\"MelbourneAirport\") | (canberra_rainfall_df[\"Location\"]==\"PerthAirport\") ]\n",
    "\n",
    "\n",
    "plt.title(\"Humidity vs Rainfall of 3 Australian Locations (Clustered, with many outliers)\")\n",
    "plt.xlabel('Humidity 3pm', fontsize=18)\n",
    "plt.ylabel('Rainfall', fontsize=16)\n",
    "for name,group in canberra_rainfall_df.groupby(\"Location\"):\n",
    "    plt.scatter(group[\"Humidity3pm\"],group[\"Rainfall\"],label=name)\n",
    "    plt.legend()\n",
    "\n",
    "    \n",
    "    \n",
    "plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "canberra_rainfall_df = canberra_rainfall_df[(canberra_rainfall_df[\"Location\"]==\"Portland\")  | (canberra_rainfall_df[\"Location\"]==\"MelbourneAirport\") ]\n",
    "\n",
    "plt.scatter(canberra_rainfall_df[\"Humidity3pm\"],canberra_rainfall_df[\"Rainfall\"])\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.title(\"Humidity vs Rainfall of 2 Australian Locations (Clustered, with few outliers)\")\n",
    "plt.xlabel('Humidity 3pm', fontsize=18)\n",
    "plt.ylabel('Rainfall', fontsize=16)\n",
    "for name,group in canberra_rainfall_df.groupby(\"Location\"):\n",
    "    plt.scatter(group[\"Humidity3pm\"],group[\"Rainfall\"],label=name)\n",
    "    plt.legend()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data NORMALIZATION\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mpl_toolkits import mplot3d\n",
    "import copy\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"ausweather_preprocessed.csv\",sep=\"\\t\")\n",
    "\n",
    "\n",
    "grouped_by_month_rainfall = df.groupby([df.Date.str[:11],\"Location\"])[\"Location\",\"MaxTemp\",\"Humidity9am\"].mean().reset_index()\n",
    "\n",
    "#print grouped_by_month_rainfall\n",
    "grouped_2013 = grouped_by_month_rainfall\n",
    "grouped_2013 = grouped_by_month_rainfall[ (grouped_2013[\"Location\"]==\"Katherine\")  | (grouped_2013[\"Location\"]==\"Bendigo\") ]\n",
    "\n",
    "#print grouped_2013\n",
    "\n",
    "\n",
    "\n",
    "#plt.figure(figsize=(15,8))\n",
    "#plt.title(\"Humidity vs Max Temperature of Bendigo\")\n",
    "#plt.xlabel('Humidity', foanalysis / comparison of algorithmsntsize=18)\n",
    "#plt.ylabel('Max Temperature', fontsize=16)\n",
    "#EXAMPLE OF A SIMPLE TWO DIVISION CLUSTER. DATA FROM EITHER ONE OF THE SOURCES CAN BE CLASSIFIED TO EITHER ONE BASED ON SOME CLASSIFICATION ALGORITHM\n",
    "k = 0\n",
    "\n",
    "for name,group in grouped_2013.groupby(\"Location\"):\n",
    "    if(k>0):#analysis  comparison of algorithms\n",
    "        break\n",
    "    humidity_points = group[\"Humidity9am\"]\n",
    "    maxtemp_points = group[\"MaxTemp\"]\n",
    "    #plt.scatter(group[\"Humidity9am\"],group[\"MaxTemp\"])\n",
    "    k+=1\n",
    "\n",
    "\n",
    "humid = np.array(humidity_points)\n",
    "temper = np.array(maxtemp_points)\n",
    "\n",
    "humid_normal = humid\n",
    "temper_normal = temper\n",
    "\n",
    "humid_max = humid.max()\n",
    "humid_min = humid.min()\n",
    "humid_avg = humid.mean()\n",
    "\n",
    "temper_max = temper.max()\n",
    "temper_min = temper.min()\n",
    "temper_avg = temper.mean()\n",
    "\n",
    "\n",
    "for i in range(len(humid)):\n",
    "    humid[i] = (humid[i] - humid_avg)/(humid_max - humid_min)\n",
    "    \n",
    "for i in range(len(temper)):\n",
    "    temper[i] = (temper[i] - temper_avg)/(temper_max - temper_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"done\")\n",
    "humid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA VISUALIZALTION\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.scatter(humid,temper)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# READING AND ANALYZING THE PLOTS\n",
    "\n",
    "We know from the above plots that a lot of the data can be used for regression and for classification. Primarily the best ways to use the data are as follows \n",
    "\n",
    "- Prediction of Rainfall using Humidity and Temperature\n",
    "- Prediction of Next day Rainfall or Not using the conditions of the current day\n",
    "- Predictions of Humidity with other features\n",
    "- Prediction of Location using all the features. This would be a perfect MultiClassification Problem.\n",
    "\n",
    "The plots clearly show how rainfall is dependant on temperature and humidity. We can assimilate this not only from our domain knowledge, but also on the data. We also notice that different places show different patterns of data during the entire year and definitely through the years. We consider almost 80k rows of data from over the years, and notice that this problem can easily be solvable, and we can apply classification and regression in the right way to get good excerpts from this data.\n",
    "\n",
    "The different data we have comprise of large and small datasets, and we can use all these in the right proportions to tune our neural network to get the best out of the data.\n",
    "\n",
    "## Regression Plots\n",
    "\n",
    "The regression plots we have are of different kinds, we notice some with large amounts of data, and some with much less data than others. We ideally are given to understand that this data can be fitted by linear regression BUT that won't be the best fit to the data, and that a non-linear approach suits the data best. \n",
    "\n",
    "We don't leverage polynomial regression for this, but we use neural networks to give us the best fit of the data. Some of the data we see are very large in size too, such as the above humidity vs temperature data which is being used. It can clearly have many complex ways of being fit in a training. \n",
    "\n",
    "The classification data too (as shown in the 3D plot above) has many classes, and cannot be perfectly fit by a linear model, and would benefit from a complex non-linear or polynomial fit. The best way that this is done, is using a neural net with a hidden layer and an activation function which works on the data.\n",
    "\n",
    "# Classification plots\n",
    "\n",
    "The classification data, is done between two different locations in Australia. One of the plots is the differences in Rainfall and Temperature between the cities of Bendigo and Katherine. One other is the differeces between Portland and Melbourne Airport. These are different data, which although can be classified with a linear decision boundary, won't have the best results if that is done. The best way in classifying this data, would be to use Neural nets and non linear decision boundaries which would give the best fit to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III.B Nonlinear Regression \n",
    "\n",
    "- Nonlinear regression is a regression in which the dependent or criterion variables are modeled as a non-linear function of model parameters and one or more independent variables.\n",
    "\n",
    "In a nonlinear regression model we don't only consider polynomials of simple order, but also those with higher order which are way more complex. This helps the data to be fit a lot better than it would when done linearly (equivalent version of a single layer after the input layer). It can be proved that a certain amount of data which is not linear in nature can always be perfectly fit with the right neural net. A multi layer neuron can always represent any curve using the weights. Generally a hidden layer needs to be utilized but with activation functions, which are used to give a higher order to the output.\n",
    "\n",
    "If a multilayer perceptron is used normally without an activation function, it gives an output which is linear in nature. Hence we use activation functions like (tanh, relu, leakyrelu) the use of each of which gives us a different \n",
    "\n",
    "\\begin{equation}\n",
    "E = \\frac{1}{N} \\frac{1}{K}\\sum_{n=1}^{N} \\sum_{k=1}^{K} (t_{nk} - y_{nk})^2\n",
    "\\end{equation}\n",
    "\n",
    "The above error function is used to calculate error across each neuron in each layer. This is used during the backprop algorithm, wherein we try to optimize the weights using different methods. In our given code, we use an optimizer with the variable niter, which controls the number of iterations necessary. The following hyperparameters can be controlled:\n",
    "\n",
    "1) Learning Rate <br>\n",
    "2) Regularization Factor <br>\n",
    "3) Neural Net Neurons in Layers and Activation Functions. <br>\n",
    "4) Controlling the number of iterations/epochs. <br>\n",
    "\n",
    "When we speak about Non Linear Regression to fit the data, we talk about a single neuron output, rather than a set of neurons, where each neuron indicates the activation for that particular class. In this case, the single neuron output gives the predicted output for a given number of input samples. We focus on the complexity of the nonlinear function using the hyperparameter regularization which helps us attain the perfect fit for our data, without much overfitting.\n",
    "\n",
    "The learning rate can be controlled to not let it overshoot the given minima. A perfect learning rate is neither too high nor too low. It has the right amount of activation so as to give an output which fits the data perfectly. We also notice that the number of iterations or Epochs are essential to control.\n",
    "\n",
    "Each iteration is a batch optimization, where each time a full forward and backword prop takes place on the entire data, this having a high value can cause overfitting, and a low value can cause underfitting. A perfect balance between the above hyperparameters is essential.\n",
    "\n",
    "We ideally understand that Non Linear Regression is all about controlling hyperparameters, changing ONLY the hidden layer (not output, since it is 1 exquisitely), and this process can achieve the right amount of non linearity.\n",
    "\n",
    "## CODE EXPLANATION\n",
    "\n",
    "- The code is split into the following parts \n",
    "    - initialization\n",
    "    - add ones\n",
    "    - get_n layers\n",
    "    - self hunit\n",
    "    - pack\n",
    "    - unpack\n",
    "    - cp weight\n",
    "    - RBF \n",
    "    - forward\n",
    "    - backward\n",
    "    - errorf\n",
    "    - objectf\n",
    "    - train \n",
    "    - use\n",
    "    \n",
    "- MODEL\n",
    "    - All of the neural networks in this assignment exclusively use a neural network with a single hidden layer. The idea is to increase the number of hidden neurons and map it to the number of correct output layers.\n",
    "    - Each neural network model has a two step process, forward and backward propogation. \n",
    "    - FORWARD PROP: Forward Propogation is the part, where the weights are used to make the hypothesis , and predict an output using the previously updated weights. We also need to take into consideration , that this doesn't just update weights in the final layer, but all the layers and the forward propogation does that . \n",
    "    - BACKWARD PROP: The backward propogation is the part where the weights are udpated in each iteration by using an optimizer like gradient descent. The backward propogation takes each layer, and updates it using the output at that particular point. \n",
    "    - The two above process happen alternately for multiple iterations/epochs. Each of these work with the optimizer to improve the performance.\n",
    "    - First the the initialization step, where the input, hidden and output nodes are initialized.\n",
    "    - The add ones, adds the necessary non parameter ones which consist of the bias unit\n",
    "    - The objectF function gives out the error approximation. Int he case of Non Linear Regression the error function used is just Root Mean Squared error, based on which the weights are updated, and the weight improvement is selected and the system is optimized based on it's differential which updates the weights in a way that it improves gradually each time with a given learning rate.\n",
    "    - The train function consists of the optimization, and the gradient descent which is used to call the forward and backprops for niters respectively.\n",
    "    - The lambda parameter is what is most important in terms of reducing or increasing(penalizing) the complexity of the possible polynomial. \n",
    "    - If the lambda parameter is truly low, then it will allow any amount of higher order polynomial with the constraint of a given neural net but so as to NOT overfit the data.\n",
    "    - The use part of the code, is to use the weights to give a hypothesis for a given set of inputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from grad import scg, steepest\n",
    "import copy\n",
    "from util import Standardizer\n",
    "\n",
    "\n",
    "class NeuralNet:\n",
    "    def __init__(self, nunits):\n",
    "        self._nLayers=len(nunits)-1\n",
    "        self.rho = [1] * self._nLayers\n",
    "        self._W = []\n",
    "        wdims = []\n",
    "        lenweights = 0\n",
    "        for i in range(self._nLayers):\n",
    "            nwr = nunits[i] + 1\n",
    "            nwc = nunits[i+1]\n",
    "            wdims.append((nwr, nwc))\n",
    "            lenweights = lenweights + nwr * nwc\n",
    "\n",
    "        self._weights = np.random.uniform(-0.1,0.1, lenweights) \n",
    "        start = 0  # fixed index error 20110107\n",
    "        for i in range(self._nLayers):\n",
    "            end = start + wdims[i][0] * wdims[i][1] \n",
    "            self._W.append(self._weights[start:end])\n",
    "            self._W[i].resize(wdims[i])\n",
    "            start = end\n",
    "\n",
    "        self.stdX = None\n",
    "        self.stdT = None\n",
    "        self.stdTarget = True\n",
    "\n",
    "    def add_ones(self, w):\n",
    "        return np.hstack((np.ones((w.shape[0], 1)), w))\n",
    "\n",
    "    def get_nlayers(self):\n",
    "        return self._nLayers\n",
    "\n",
    "    def set_hunit(self, w):\n",
    "        for i in range(self._nLayers-1):\n",
    "            if w[i].shape != self._W[i].shape:\n",
    "                print(\"set_hunit: shapes do not match!\")\n",
    "                break\n",
    "            else:\n",
    "                self._W[i][:] = w[i][:]\n",
    "\n",
    "    def pack(self, w):\n",
    "        return np.hstack(map(np.ravel, w))\n",
    "\n",
    "    def unpack(self, weights):\n",
    "        self._weights[:] = weights[:]  # unpack\n",
    "\n",
    "    def cp_weight(self):\n",
    "        return copy.copy(self._weights)\n",
    "\n",
    "    def RBF(self, X, m=None,s=None):\n",
    "        if m is None: m = np.mean(X)\n",
    "        if s is None: s = 2 #np.std(X)\n",
    "        r = 1. / (np.sqrt(2*np.pi)* s)  \n",
    "        return r * np.exp(-(X - m) ** 2 / (2 * s ** 2))\n",
    "\n",
    "    def forward(self,X):\n",
    "        t = X \n",
    "        Z = []\n",
    "\n",
    "        for i in range(self._nLayers):\n",
    "            Z.append(t) \n",
    "            if i == self._nLayers - 1:\n",
    "                t = np.dot(self.add_ones(t), self._W[i])\n",
    "            else:\n",
    "                t = np.tanh(np.dot(self.add_ones(t), self._W[i]))\n",
    "                #t = self.RBF(np.dot(np.hstack((np.ones((t.shape[0],1)),t)),self._W[i]))\n",
    "        return (t, Z)\n",
    "        \n",
    "    def backward(self, error, Z, T, lmb=0):\n",
    "        delta = error\n",
    "        N = T.size\n",
    "        dws = []\n",
    "        for i in range(self._nLayers - 1, -1, -1):\n",
    "            rh = float(self.rho[i]) / N\n",
    "            if i==0:\n",
    "                lmbterm = 0\n",
    "            else:\n",
    "                lmbterm = lmb * np.vstack((np.zeros((1, self._W[i].shape[1])),\n",
    "                            self._W[i][1:,]))\n",
    "            dws.insert(0,(-rh * np.dot(self.add_ones(Z[i]).T, delta) + lmbterm))\n",
    "            if i != 0:\n",
    "                #print(delta)\n",
    "                #print(\"p2\")\n",
    "                #print(Z)\n",
    "                #print(\"p3\")\n",
    "                #print(self._W[i][1:, :].T)\n",
    "                delta = np.dot(delta, self._W[i][1:, :].T) * (1 - Z[i]**2)\n",
    "        return self.pack(dws)\n",
    "\n",
    "    def _errorf(self, T, Y):\n",
    "        return T - Y\n",
    "        \n",
    "    def _objectf(self, T, Y, wpenalty):\n",
    "        return 0.5 * np.mean(np.square(T - Y)) + wpenalty\n",
    "\n",
    "    def train(self, X, T,**params):\n",
    "        verbose = params.pop('verbose', False)\n",
    "        # training parameters\n",
    "        _lambda = params.pop('Lambda', 0)\n",
    "\n",
    "        #parameters for scg\n",
    "        niter = params.pop('niter', 1000)\n",
    "        wprecision = params.pop('wprecision', 1e-10)\n",
    "        fprecision = params.pop('fprecision', 1e-10)\n",
    "        wtracep = params.pop('wtracep', False)\n",
    "        ftracep = params.pop('ftracep', False)\n",
    "\n",
    "        # optimization\n",
    "        optim = params.pop('optim', 'scg')\n",
    "\n",
    "        if self.stdX == None:\n",
    "            explore = params.pop('explore', False)\n",
    "            self.stdX = Standardizer(X, explore)\n",
    "        Xs = self.stdX.standardize(X)\n",
    "        if self.stdT == None and self.stdTarget:\n",
    "            self.stdT = Standardizer(T)\n",
    "            T = self.stdT.standardize(T)\n",
    "        \n",
    "        def gradientf(weights):\n",
    "            self.unpack(weights)\n",
    "            Y,Z = self.forward(Xs)\n",
    "            error = self._errorf(T, Y)\n",
    "            return self.backward(error, Z, T, _lambda)\n",
    "            \n",
    "        def optimtargetf(weights):\n",
    "            \"\"\" optimization target function : MSE \n",
    "            \"\"\"\n",
    "            self.unpack(weights)\n",
    "            #self._weights[:] = weights[:]  # unpack\n",
    "            Y,_ = self.forward(Xs)\n",
    "            Wnb=np.array([])\n",
    "            for i in range(self._nLayers):\n",
    "                if len(Wnb)==0: Wnb=self._W[i][1:,].reshape(self._W[i].size-self._W[i][0,].size,1)\n",
    "                else: Wnb = np.vstack((Wnb,self._W[i][1:,].reshape(self._W[i].size-self._W[i][0,].size,1)))\n",
    "            wpenalty = _lambda * np.dot(Wnb.flat ,Wnb.flat)\n",
    "            return self._objectf(T, Y, wpenalty)\n",
    "\n",
    "        if optim == 'scg':\n",
    "            result = scg(self.cp_weight(), gradientf, optimtargetf,\n",
    "                                        wPrecision=wprecision, fPrecision=fprecision, \n",
    "                                        nIterations=niter,\n",
    "                                        wtracep=wtracep, ftracep=ftracep,\n",
    "                                        verbose=False)\n",
    "            self.unpack(result['w'][:])\n",
    "            self.f = result['f']\n",
    "        elif optim == 'steepest':\n",
    "            result = steepest(self.cp_weight(), gradientf, optimtargetf,\n",
    "                                nIterations=niter,\n",
    "                                xPrecision=wprecision, fPrecision=fprecision,\n",
    "                                xtracep=wtracep, ftracep=ftracep )\n",
    "            self.unpack(result['w'][:])\n",
    "        if ftracep:\n",
    "            self.ftrace = result['ftrace']\n",
    "        if 'reason' in result.keys() and verbose:\n",
    "            print(result['reason'])\n",
    "\n",
    "        return result\n",
    "\n",
    "    def use(self, X, retZ=False):\n",
    "        if self.stdX:\n",
    "            Xs = self.stdX.standardize(X)\n",
    "        else:\n",
    "            Xs = X\n",
    "        Y, Z = self.forward(Xs)\n",
    "        if self.stdT is not None:\n",
    "            Y = self.stdT.unstandardize(Y)\n",
    "        if retZ:\n",
    "            return Y, Z\n",
    "        return Y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## II.C Nonlinear Logistic Regression\n",
    "\n",
    "### Explanation\n",
    "- Nonlinear regression is a regression in which the dependent or criterion variables are modeled as a non-linear function of model parameters and one or more independent variables.\n",
    "- The core idea behind any Non Linear method is to attain better accuracies , which can be possible only with higher than linear order polynomial which is what we are leveraging to perform our task.\n",
    "- We can choose the right higher order polynomial, in general to distinguish between our different classes of data, but this would really just hinder our task of understanding the data better, but this would require a higher level understanding of not only the data, but also the way the features interact.\n",
    "- A neural network with a multi layer perceptron, will do this task for us, and we can play with the number of neurons in each layer to find the accurate solution for our problem.\n",
    "- One way to do so, is by adding hidden layer neurons to a point where we fit the data and the case but don't overfit.\n",
    "- The core difference in the \"logistic\" non linear regression approach, is that we don't use a regular hypothsis without activating it in the final layer of the neural network. \n",
    "### - ACTIVATION FUNCTION choices: We have many choices of activation at this stage. <br>\n",
    "    - Sigmoid\n",
    "    - Reulu\n",
    "    - Softmax\n",
    "    - Leaky Relu\n",
    "    <br>\n",
    "    \n",
    "- Sigmod function activates one output against ONE other output, and give us the probability of one agains the other.\n",
    "- RELU, also takes into consideration activating for a certain kind of output but nulls out completely, when the output goes below a certain threshold\n",
    "- Softmax, is ideally used when the number of output classes is large, and we don't want to give a HARSH (hardmax) penalty for the loss, but a more gradual one hence, softmax.\n",
    "- Leaky Relu, is a variant where it doesn't completely null the value and doesn't allow it to change further.\n",
    "\n",
    "\n",
    "Using the knowledge we have, we pick a different activation function, and based on this we classify the data.\n",
    "\n",
    "In our case, we are using Non Linear Logistic Regression using Softmax, because there are multiple classes(34 classes in face). Due to this we are primed to use softmax, which divides the percentage of the losses within each of the different classes,and kind of gives a percentage of chance that the input is of which class.\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "P(y=j) = \\frac{e^{\\vec w_j \\cdot \\vec x}}{\\sum\\limits_{i=1}^{K}e^{\\vec w_i \\cdot \\vec x}}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "## CODE EXPLANATION\n",
    "\n",
    "- The code is split into the following parts \n",
    "    - initialization\n",
    "    - add ones\n",
    "    - get_n layers\n",
    "    - self hunit\n",
    "    - pack\n",
    "    - unpack\n",
    "    - cp weight\n",
    "    - RBF \n",
    "    - forward\n",
    "    - backward\n",
    "    - errorf\n",
    "    - objectf\n",
    "    - train \n",
    "    - use\n",
    "    \n",
    "- MODEL\n",
    "    - All of the neural networks in this assignment exclusively use a neural network with a single hidden layer. The idea is to increase the number of hidden neurons and map it to the number of correct output layers.\n",
    "    - Each neural network model has a two step process, forward and backward propogation. \n",
    "    - FORWARD PROP: Forward Propogation is the part, where the weights are used to make the hypothesis , and predict an output using the previously updated weights. We also need to take into consideration , that this doesn't just update weights in the final layer, but all the layers and the forward propogation does that . \n",
    "    - BACKWARD PROP: The backward propogation is the part where the weights are udpated in each iteration by using an optimizer like gradient descent. The backward propogation takes each layer, and updates it using the output at that particular point. \n",
    "    - The two above process happen alternately for multiple iterations/epochs. Each of these work with the optimizer to improve the performance.\n",
    "    - First the the initialization step, where the input, hidden and output nodes are initialized.\n",
    "    - The add ones, adds the necessary non parameter ones which consist of the bias unit\n",
    "    - The objectF function gives out the error approximation. Int he case of Non Linear Regression the error function used is just log error, based on which the weights are updated, and the weight improvement is selected and the system is optimized based on it's differential which updates the weights in a way that it improves gradually each time with a given learning rate.The log error loss is the best for logistic regression when we choose to use sigmoid or softmax regression.\n",
    "    - The train function consists of the optimization, and the gradient descent which is used to call the forward and backprops for niters respectively.\n",
    "    - The lambda parameter is what is most important in terms of reducing or increasing(penalizing) the complexity of the possible polynomial. \n",
    "    - If the lambda parameter is truly low, then it will allow any amount of higher order polynomial with the constraint of a given neural net but so as to NOT overfit the data.\n",
    "    - The use part of the code, is to use the weights to give a hypothesis for a given set of inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from grad import scg, steepest\n",
    "import copy\n",
    "from util import Standardizer\n",
    "\n",
    "\n",
    "class NeuralNetLog:\n",
    "    def __init__(self, nunits):\n",
    "        self._nLayers=len(nunits)-1\n",
    "        self.rho = [1] * self._nLayers\n",
    "        self._W = []\n",
    "        wdims = []\n",
    "        lenweights = 0\n",
    "        for i in range(self._nLayers):\n",
    "            nwr = nunits[i] + 1\n",
    "            nwc = nunits[i+1]\n",
    "            wdims.append((nwr, nwc))\n",
    "            lenweights = lenweights + nwr * nwc\n",
    "\n",
    "        self._weights = np.random.uniform(-0.1,0.1, lenweights) \n",
    "        start = 0  # fixed index error 20110107\n",
    "        for i in range(self._nLayers):\n",
    "            end = start + wdims[i][0] * wdims[i][1] \n",
    "            self._W.append(self._weights[start:end])\n",
    "            self._W[i].resize(wdims[i])\n",
    "            start = end\n",
    "\n",
    "        self.stdX = None\n",
    "        self.stdT = None\n",
    "        self.stdTarget = True\n",
    "\n",
    "    def add_ones(self, w):\n",
    "        return np.hstack((np.ones((w.shape[0], 1)), w))\n",
    "\n",
    "    def get_nlayers(self):\n",
    "        return self._nLayers\n",
    "\n",
    "    def set_hunit(self, w):\n",
    "        for i in range(self._nLayers-1):\n",
    "            if w[i].shape != self._W[i].shape:\n",
    "                print(\"set_hunit: shapes do not match!\")\n",
    "                break\n",
    "            else:\n",
    "                self._W[i][:] = w[i][:]\n",
    "\n",
    "    def pack(self, w):\n",
    "        return np.hstack(map(np.ravel, w))\n",
    "\n",
    "    def unpack(self, weights):\n",
    "        self._weights[:] = weights[:]  # unpack\n",
    "\n",
    "    def cp_weight(self):\n",
    "        return copy.copy(self._weights)\n",
    "\n",
    "    def RBF(self, X, m=None,s=None):\n",
    "        if m is None: m = np.mean(X)\n",
    "        if s is None: s = 2 #np.std(X)\n",
    "        r = 1. / (np.sqrt(2*np.pi)* s)  \n",
    "        return r * np.exp(-(X - m) ** 2 / (2 * s ** 2))\n",
    "\n",
    "    def forward(self,X):\n",
    "        t = X \n",
    "        Z = []\n",
    "\n",
    "        for i in range(self._nLayers):\n",
    "            Z.append(t) \n",
    "            if i == self._nLayers - 1:\n",
    "                #t = np.tanh(np.dot(self.add_ones(t), self._W[i]))\n",
    "                #t = np.dot(self.add_ones(t), self._W[i])\n",
    "                #expmat = np.exp(np.dot(self.add_ones(t), self._W[i]))\n",
    "                #print(expmat.shape)\n",
    "                #denom = np.sum(expmat,axis=0)\n",
    "                #t = expmat/denom\n",
    "                t = 1/(1+np.exp(-np.dot(self.add_ones(t), self._W[i])))\n",
    "                \n",
    "                #print(t)\n",
    "            else:\n",
    "                t = np.tanh(np.dot(self.add_ones(t), self._W[i]))\n",
    "                #t = self.RBF(np.dot(np.hstack((np.ones((t.shape[0],1)),t)),self._W[i]))\n",
    "        return (t, Z)\n",
    "        \n",
    "    def backward(self, error, Z, T, lmb=0):\n",
    "        delta = error\n",
    "        N = T.size\n",
    "        dws = []\n",
    "        for i in range(self._nLayers - 1, -1, -1):\n",
    "            rh = float(self.rho[i]) / N\n",
    "            if i==0:\n",
    "                lmbterm = 0\n",
    "            else:\n",
    "                lmbterm = lmb * np.vstack((np.zeros((1, self._W[i].shape[1])),\n",
    "                            self._W[i][1:,]))\n",
    "            dws.insert(0,(-rh * np.dot(self.add_ones(Z[i]).T, delta) + lmbterm))\n",
    "            if i != 0:\n",
    "                #print(delta)\n",
    "                #print(\"p2\")\n",
    "                #print(Z)\n",
    "                #print(\"p3\")\n",
    "                #print(self._W[i][1:, :].T)\n",
    "                delta = np.dot(delta, self._W[i][1:, :].T) * (1 - Z[i]**2)\n",
    "        return self.pack(dws)\n",
    "\n",
    "    def _errorf(self, T, Y):\n",
    "        return T - Y\n",
    "        \n",
    "    def _objectf(self, T, Y, wpenalty):\n",
    "        return -(np.sum( np.sum((T * np.log(Y)) , axis=1), axis=0)) + wpenalty\n",
    "\n",
    "    def train(self, X, T,**params):\n",
    "        verbose = params.pop('verbose', False)\n",
    "        # training parameters\n",
    "        _lambda = params.pop('Lambda', 0)\n",
    "\n",
    "        #parameters for scg\n",
    "        niter = params.pop('niter', 1000)\n",
    "        wprecision = params.pop('wprecision', 1e-10)\n",
    "        fprecision = params.pop('fprecision', 1e-10)\n",
    "        wtracep = params.pop('wtracep', False)\n",
    "        ftracep = params.pop('ftracep', False)\n",
    "\n",
    "        # optimization\n",
    "        optim = params.pop('optim', 'scg')\n",
    "\n",
    "        if self.stdX == None:\n",
    "            explore = params.pop('explore', False)\n",
    "            self.stdX = Standardizer(X, explore)\n",
    "        Xs = self.stdX.standardize(X)\n",
    "        if self.stdT == None and self.stdTarget and False:\n",
    "            self.stdT = Standardizer(T)\n",
    "            T = self.stdT.standardize(T)\n",
    "        \n",
    "        def gradientf(weights):\n",
    "            self.unpack(weights)\n",
    "            Y,Z = self.forward(Xs)\n",
    "            error = self._errorf(T, Y)\n",
    "            return self.backward(error, Z, T, _lambda)\n",
    "            \n",
    "        def optimtargetf(weights):\n",
    "            \"\"\" optimization target function : MSE \n",
    "            \"\"\"\n",
    "            self.unpack(weights)\n",
    "            #self._weights[:] = weights[:]  # unpack\n",
    "            Y,_ = self.forward(Xs)\n",
    "            Wnb=np.array([])\n",
    "            for i in range(self._nLayers):\n",
    "                if len(Wnb)==0: Wnb=self._W[i][1:,].reshape(self._W[i].size-self._W[i][0,].size,1)\n",
    "                else: Wnb = np.vstack((Wnb,self._W[i][1:,].reshape(self._W[i].size-self._W[i][0,].size,1)))\n",
    "            wpenalty = _lambda * np.dot(Wnb.flat ,Wnb.flat)\n",
    "            return self._objectf(T, Y, wpenalty)\n",
    "\n",
    "        if optim == 'scg':\n",
    "            result = scg(self.cp_weight(), gradientf, optimtargetf,\n",
    "                                        wPrecision=wprecision, fPrecision=fprecision, \n",
    "                                        nIterations=niter,\n",
    "                                        wtracep=wtracep, ftracep=ftracep,\n",
    "                                        verbose=False)\n",
    "            self.unpack(result['w'][:])\n",
    "            self.f = result['f']\n",
    "        elif optim == 'steepest':\n",
    "            result = steepest(self.cp_weight(), gradientf, optimtargetf,\n",
    "                                nIterations=niter,\n",
    "                                xPrecision=wprecision, fPrecision=fprecision,\n",
    "                                xtracep=wtracep, ftracep=ftracep )\n",
    "            self.unpack(result['w'][:])\n",
    "        if ftracep:\n",
    "            self.ftrace = result['ftrace']\n",
    "        if 'reason' in result.keys() and verbose:\n",
    "            print(result['reason'])\n",
    "\n",
    "        return result\n",
    "\n",
    "    def use(self, X, retZ=False):\n",
    "        if self.stdX:\n",
    "            Xs = self.stdX.standardize(X)\n",
    "        else:\n",
    "            Xs = X\n",
    "        Y, Z = self.forward(Xs)\n",
    "        if self.stdT is not None:\n",
    "            Y = self.stdT.unstandardize(Y)\n",
    "        if retZ:\n",
    "            return Y, Z\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examination of correct implementation (NonlinearLogReg) with toy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mpl_toolkits import mplot3d\n",
    "import copy\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"data.csv\",sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"done\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scipy.stats import itemfreq\n",
    "#np.asmatrix(humid).shape[0]\n",
    "#np.asmatrix(temper).shape[0]\n",
    "#copy.copy([1,2,3])\n",
    "print(df.values.shape)\n",
    "groupwithout.ix[:,[2,3,4]]\n",
    "#groupwithout.ix[:,[0:3]]\n",
    "X = df.ix[:,[1,3,4,5,6,7,8,9,10]].values\n",
    "X = X[:35000,:]\n",
    "T = df.ix[:,[2]].values\n",
    "\n",
    "T = T[:35000,:]\n",
    "print(type(T))\n",
    "\n",
    "\n",
    "classes = np.unique(T)\n",
    "numclasses = len(np.unique(T))\n",
    "print(\"numclasses\")\n",
    "print(numclasses)\n",
    "base_class = np.array([0 for i in range(numclasses)])\n",
    "\n",
    "new_T = np.array([base_class]*T.shape[0])\n",
    "\n",
    "print(new_T)\n",
    "print(new_T.shape)\n",
    "\n",
    "for i in range(X.shape[0]):\n",
    "    #print(T[i])\n",
    "    #print(np.where(classes==T[i]))\n",
    "    new_T[i][np.where(classes==T[i])] = 1\n",
    "    \n",
    "\n",
    " \n",
    "\n",
    "print(new_T)\n",
    "print(new_T.shape)\n",
    "trainnet = NeuralNetLog([X.shape[1],20,numclasses])\n",
    "trainnet.train(X,new_T,ftracep=True)\n",
    "#tranans,z = trainnet.use(X,retZ=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "X, T = make_circles(n_samples=800, noise=0.07, factor=0.4)\n",
    "new_T = np.array([[0,0]]*X.shape[0]) #one hot T\n",
    "for i in range(len(T)):\n",
    "    new_T[i][T[i]] = 1\n",
    "    \n",
    "print(X)\n",
    "print(new_T)\n",
    "\n",
    "plt.scatter(X[:,0],X[:,1])\n",
    "\n",
    "trainnet = NeuralNetLog([X.shape[1],11,2])\n",
    "trainnet.train(X,new_T,ftracep=True)\n",
    "tranans,z = trainnet.use(X,retZ=True)\n",
    "\n",
    "#cc = CrossValid(X,new_T)\n",
    "#cc.five_fold_classify()\n",
    "\n",
    "#new_T = np.\n",
    "print(tranans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xans = np.append(X,tranans.argmax(axis=1).reshape(len(tranans),1),1)\n",
    "#Xans = np.append(X,tranans,1)\n",
    "print(T.shape)\n",
    "Xtrain = np.append(X,T.reshape(len(T),1),1)\n",
    "\n",
    "print(X)\n",
    "print(X.shape)\n",
    "n = np.unique(Xtrain[:,2])\n",
    "groupednp = np.array( [ list(Xtrain[Xtrain[:,2]==i,0]) for i in n])\n",
    "groupednp2 = np.array( [ list(Xtrain[Xtrain[:,2]==i,1]) for i in n])\n",
    "\n",
    "n = np.unique(Xans[:,2])\n",
    "groupednpans = np.array( [ list(Xans[Xans[:,2]==i,0]) for i in n])\n",
    "groupednpans2 = np.array( [ list(Xans[Xans[:,2]==i,1]) for i in n])\n",
    "\n",
    "print(groupednp.shape)\n",
    "print(groupednpans.shape)\n",
    "print(groupednpans2.shape)\n",
    "\n",
    "i=1\n",
    "\n",
    "\n",
    "colors = \"bgrcmyk\"\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "plt.title(\"ACTUAL MULTI DIMENSION ACTUALIZED IN TWO DIMENSIONS\")\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "#color = colors[T.argmax(0)]\n",
    "for i in range(len(groupednp)):\n",
    "    groupednp[i] = (np.array(groupednp[i]) - np.array(groupednp[i]).min())/(np.array(groupednp[i]).max() - np.array(groupednp[i]).min())\n",
    "    groupednp2[i] = (np.array(groupednp2[i]) - np.array(groupednp2[i]).min())/(np.array(groupednp2[i]).max() - np.array(groupednp2[i]).min())\n",
    "    ax1.scatter(groupednp[i],groupednp2[i])\n",
    "    \n",
    "#fig = plt.figure(figsize=(15,8))\n",
    "plt.title(\"PREDICTED MULTI DIMENSION ACTUALIZED IN TWO DIMENSIONS\")\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "#color = colors[T.argmax(0)]\n",
    "for i in range(len(groupednpans)):\n",
    "    groupednpans[i] = (np.array(groupednpans[i]) - np.array(groupednpans[i]).min())/(np.array(groupednpans[i]).max() - np.array(groupednpans[i]).min())\n",
    "    groupednpans2[i] = (np.array(groupednpans2[i]) - np.array(groupednpans2[i]).min())/(np.array(groupednpans2[i]).max() - np.array(groupednpans2[i]).min())\n",
    "    \n",
    "    #ax1.scatter(groupednpans[i],groupednpans2[i])\n",
    "    \n",
    "\n",
    "    \n",
    "#plt.plot(humid,Ytest,color=\"orange\")\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification works, but is misclassifying, since the hyperparameters are tuned to the actual input of the large dataset being used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import itemfreq\n",
    "#np.asmatrix(humid).shape[0]\n",
    "#np.asmatrix(temper).shape[0]\n",
    "#copy.copy([1,2,3])\n",
    "print(df.values.shape)\n",
    "groupwithout.ix[:,[2,3,4]]\n",
    "#groupwithout.ix[:,[0:3]]\n",
    "X = df.ix[:,[3,4,5,7,10,11,12,13,14,15,16,17,18,19,21]].values\n",
    "X = X[:35000,:]\n",
    "T = df.ix[:,[20]].values\n",
    "\n",
    "T = T[:35000,:]\n",
    "print(type(T))\n",
    "\n",
    "\n",
    "classes = np.unique(T)\n",
    "numclasses = len(np.unique(T))\n",
    "\n",
    "base_class = np.array([0 for i in range(numclasses)])\n",
    "\n",
    "new_T = np.array([base_class]*T.shape[0])\n",
    "\n",
    "print(new_T)\n",
    "print(new_T.shape)\n",
    "\n",
    "for i in range(X.shape[0]):\n",
    "    #print(T[i])\n",
    "    #print(np.where(classes==T[i]))\n",
    "    new_T[i][np.where(classes==T[i])] = 1\n",
    "    \n",
    "\n",
    " \n",
    "\n",
    "print(new_T)\n",
    "print(new_T.shape)\n",
    "trainnet = NeuralNetLog([X.shape[1],20,numclasses])\n",
    "trainnet.train(X,new_T,ftracep=True)\n",
    "tranans,z = trainnet.use(X,retZ=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "X_max = X.max(axis=0)\n",
    "X_min = X.min(axis=0)\n",
    "X_avg = X.mean(axis=0)\n",
    "\n",
    "T_max = T.max()\n",
    "T_min = T.min()\n",
    "T_avg = T.mean()\n",
    "\n",
    "\n",
    "print(X_max.shape)\n",
    "    \n",
    "    ax1.scatter(groupednpans[i],groupednpans2[i])\n",
    "for i in range(X.shape[0]):\n",
    "    #print(X[i,:].shape)\n",
    "    #print(X_avg.shape)\n",
    "    #print(X_min.shape)\n",
    "    #print(X_max.shape)\n",
    "    X[i,:] = (X[i,:] - X_avg)/(X_max - X_min)\n",
    "print(X.shape)\n",
    "    \n",
    "for i in range(len(T)):7\n",
    "    T[i] = (T[i] - T_avg)/(T_max - T_min)\n",
    "    \n",
    "print(X.shape)\n",
    "print(T.shape)\n",
    "#groupwithout[\"MaxTemp\"],groupwithout[\"Humidity9am\"],groupwithout[\"Rainfall\"]\n",
    "#trainnet = NeuralNet([X.shape[1],10,1])\n",
    "#trainnet.train(X,T,ftracep=True)\n",
    "\n",
    "\n",
    "#answers,z = trainnet.use(X, retZ=True)\n",
    "cc = CrossValid(X,T)\n",
    "cc.five_fold_regress()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Done\")\n",
    "print(tranans[12141])\n",
    "print(len(np.unique(tranans,axis=1)))\n",
    "print(np.unique(tranans.argmax(axis=1)))\n",
    "print(tranans.shape)\n",
    "print(tranans.argmax(axis=1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xans = np.append(X,tranans.argmax(axis=1).reshape(len(tranans),1),1)\n",
    "#Xans = np.append(X,tranans,1)\n",
    "Xtrain = np.append(X,T,1)\n",
    "\n",
    "print(X)\n",
    "print(X.shape)\n",
    "n = np.unique(Xtrain[:,15])\n",
    "groupednp = np.array( [ list(Xtrain[Xtrain[:,15]==i,1]) for i in n])\n",
    "groupednp2 = np.array( [ list(Xtrain[Xtrain[:,15]==i,2]) for i in n])\n",
    "\n",
    "n = np.unique(Xans[:,15])\n",
    "groupednpans = np.array( [ list(Xans[Xans[:,15]==i,1]) for i in n])\n",
    "groupednpans2 = np.array( [ list(Xans[Xans[:,15]==i,2]) for i in n])\n",
    "\n",
    "print(groupednp.shape)\n",
    "print(groupednpans.shape)\n",
    "print(groupednpans2.shape)\n",
    "\n",
    "i=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "colors = \"bgrcmyk\"\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "plt.title(\"ACTUAL MULTI DIMENSION ACTUALIZED IN TWO DIMENSIONS\")\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "#color = colors[T.argmax(0)]\n",
    "for i in range(len(groupednp)):\n",
    "    groupednp[i] = (np.array(groupednp[i]) - np.array(groupednp[i]).min())/(np.array(groupednp[i]).max() - np.array(groupednp[i]).min())\n",
    "    groupednp2[i] = (np.array(groupednp2[i]) - np.array(groupednp2[i]).min())/(np.array(groupednp2[i]).max() - np.array(groupednp2[i]).min())\n",
    "    ax1.scatter(groupednp[i],groupednp2[i])\n",
    "    \n",
    "fig = plt.figure(figsize=(15,8))\n",
    "plt.title(\"PREDICTED MULTI DIMENSION ACTUALIZED IN TWO DIMENSIONS\")\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "#color = colors[T.argmax(0)]\n",
    "for i in range(len(groupednpans)):\n",
    "    groupednpans[i] = (np.array(groupednpans[i]) - np.array(groupednpans[i]).min())/(np.array(groupednpans[i]).max() - np.array(groupednpans[i]).min())\n",
    "    groupednpans2[i] = (np.array(groupednpans2[i]) - np.array(groupednpans2[i]).min())/(np.array(groupednpans2[i]).max() - np.array(groupednpans2[i]).min())\n",
    "    \n",
    "    ax1.scatter(groupednpans[i],groupednpans2[i])\n",
    "    \n",
    "\n",
    "    \n",
    "#plt.plot(humid,Ytest,color=\"orange\")\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(groupednpans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xans = np.append(X,tranans.argmax(axis=1).reshape(len(tranans),1),1)\n",
    "#Xans = np.append(X,tranans,1)\n",
    "Xtrain = np.append(X,T,1)\n",
    "\n",
    "print(X)\n",
    "print(X.shape)\n",
    "n = np.unique(Xtrain[:,15])\n",
    "groupednp = np.array( [ list(Xtrain[Xtrain[:,15]==i,2]) for i in n])\n",
    "groupednp2 = np.array( [ list(Xtrain[Xtrain[:,15]==i,3]) for i in n])\n",
    "\n",
    "n = np.unique(Xans[:,15])\n",
    "groupednpans = np.array( [ list(Xans[Xans[:,15]==i,2]) for i in n])\n",
    "groupednpans2 = np.array( [ list(Xans[Xans[:,15]==i,3]) for i in n])\n",
    "\n",
    "print(groupednp.shape)\n",
    "print(groupednpans.shape)\n",
    "print(groupednpans2.shape)\n",
    "\n",
    "i=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "colors = \"bgrcmyk\"\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "ax1 = fig.add_subplot(111)\n",
    "plt.title(\"ACTUAL MULTI DIMENSION ACTUALIZED IN TWO DIMENSIONS\")\n",
    "#color = colors[T.argmax(0)]\n",
    "for i in range(len(groupednp)):\n",
    "    groupednp[i] = (np.array(groupednp[i]) - np.array(groupednp[i]).min())/(np.array(groupednp[i]).max() - np.array(groupednp[i]).min())\n",
    "    groupednp2[i] = (np.array(groupednp2[i]) - np.array(groupednp2[i]).min())/(np.array(groupednp2[i]).max() - np.array(groupednp2[i]).min())\n",
    "    ax1.scatter(groupednp[i],groupednp2[i])\n",
    "    \n",
    "fig = plt.figure(figsize=(15,8))\n",
    "ax1 = fig.add_subplot(111)\n",
    "plt.title(\"PREDICTED MULTI DIMENSION ACTUALIZED IN TWO DIMENSIONS\")\n",
    "#color = colors[T.argmax(0)]\n",
    "for i in range(len(groupednpans)):\n",
    "    groupednpans[i] = (np.array(groupednpans[i]) - np.array(groupednpans[i]).min())/(np.array(groupednpans[i]).max() - np.array(groupednpans[i]).min())\n",
    "    groupednpans2[i] = (np.array(groupednpans2[i]) - np.array(groupednpans2[i]).min())/(np.array(groupednpans2[i]).max() - np.array(groupednpans2[i]).min())\n",
    "    \n",
    "    ax1.scatter(groupednpans[i],groupednpans2[i])\n",
    "    \n",
    "\n",
    "    \n",
    "#plt.plot(humid,Ytest,color=\"orange\")\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## III.A 5-fold Cross Validation\n",
    "\n",
    "- Explain and use 5-fold cross validation to find a good neural network parameters including the structure to report the CV accuracies. \n",
    "The five fold cross validation is used in the following way during the training process, and during the testing process/phase. The key idea behind using the 5 - fold cross validation, is that while we perform testing and training, we simultaneously perform validation, where we train each part of the data targetted, based on which we derive a certain number of weights. <br>\n",
    "\n",
    "The key Idea behind using 5 fold cross validation is as follows: <br><br>\n",
    "1) Train in chunks, and evaluate success rate simulataneously so that good hyperparameters can be defined easily, during hte process occurs. The key idea behind validation is also to catch overfitting and to adjust the number of epochs based on whether the data actually does overfit after a point. <br>\n",
    "\n",
    "2) The second idea behind 5-fold Cross Validation, Is to split training and test data, so as to test whether the algorithm and the weights have actually fit the use case, or just the given training data. Again, the key idea here is to find the perfect fit for the data, rather than just a set of weights which match every point (overfitting). We also can prevent underfitting in this process, by increasing the number of epochs, adjusting the learning rate, based on how the performance is in the validation set. <br>\n",
    "\n",
    "3) Adjusting of hyperparameters, very often becomes the key necessity of using a neural net to classify large scale data. Again one of the methods employed here is to assess progress, and create an idea of modifying the hyperparameters, so as to find the right fit. There are a few common hyperparameters which are key which can be easily identified and dealt with during the process of Cross Validation. One of those is Identifying the regularization parameter, which penalizes the use of too many higher order polynomials. <br>\n",
    "\n",
    "4) The five fold cross validation implementation has been done in a class, which calculates the RMSE for regression and accuracy of prediction for a given classification problem.\n",
    "\n",
    "## CODE EXPLANATION\n",
    "\n",
    "The code below takes into consideration a 5 fold, in which 3 parts are used for training, 1 part is used for side-by-side validation and one is used for testing, in which the data has been split (the train data, which is already 3/5 of the total data), has now been split into smaller part ( 10000 for each chunk data). <br><br>\n",
    "\n",
    "The code also has an error function for regression and for classification,wherein, the error function for regression is a Root Mean Squared (RMSE) error function, which gives out the total error we have in a given prediction, and this is done for each of the 10,000 steps.\n",
    "\n",
    "We also initially take the type into account as to whether it is regression or classification, and apply the respective five fold and error display process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class CrossValid:\n",
    "    def __init__(self,X,Y,typ=\"reg\"):\n",
    "        self.X = X\n",
    "        self.T = Y\n",
    "        self.typ = typ\n",
    "    \n",
    "    def error_func_regress(self,myanswer,Y):\n",
    "        return (np.sum(((Y-myanswer)**2)/Y.shape[0]))**0.5\n",
    "    \n",
    "    def error_func_classify(self,T,Y):\n",
    "        return float(100 - ((np.count_nonzero(np.abs(T.argmax(axis=1) - Y.argmax(axis=1)))))*100/T.shape[0])\n",
    "            \n",
    "        \n",
    "    def five_fold_regress(self):\n",
    "        print(self.X.shape)\n",
    "        part = int(self.X.shape[0]/5)\n",
    "        train = self.X[:5*part+1,:]\n",
    "        trainanswers = self.T[:5*part+1,:]\n",
    "        validate = self.X[3*part+1:4*part+1,:]\n",
    "        validateanswers = self.T[3*part+1:4*part+1:,:]\n",
    "        test = self.X[4*part+1:5*part:,:]\n",
    "        testanswers = self.T[4*part+1:5*part:,:]\n",
    "        trainnet = NeuralNet([self.X.shape[1],3,1])\n",
    "        models = [[self.X.shape[1],2,1],[self.X.shape[1],3,1],[self.X.shape[1],11,1],[self.X.shape[1],7,1],[self.X.shape[1],12,1],[self.X.shape[1],3,1]]\n",
    "        \n",
    "        #trainnet.train(train[0:9999,:],trainanswers[0:9999,:])\n",
    "        \n",
    "        if part > 10000:\n",
    "            fold = part\n",
    "        else:\n",
    "            fold = X.shape[0]\n",
    "            part = fold\n",
    "        \n",
    "        for i in range(0,train.shape[0],fold):\n",
    "            trainnet = NeuralNet(models[int(i/fold)])\n",
    "            rn = random.randint(0,4)\n",
    "            validate = self.X[(rn*part)+1:((rn+1)*part)+1,:]\n",
    "            validateanswers = self.T[(rn*part)+1:((rn+1)*part)+1,:]\n",
    "            if train.shape[0] < i + fold-1:\n",
    "                trainnet.train(train[i:i+train.shape[0]-1,:],trainanswers[i:i+train.shape[0]-1,:],ftracep=True)\n",
    "                \n",
    "            else:\n",
    "                trainnet.train(train[i:i+fold-1,:],trainanswers[i:i+fold-1,:],ftracep=True)\n",
    "            \n",
    "            myans,z = trainnet.use(validate,retZ=True)\n",
    "            print(\"\\n -------------------------- \\n\")\n",
    "            print(\"RMSE IN PART \" + str(int(i/fold) + 1) + \" IS: \")\n",
    "            print(self.error_func_regress(myans,validateanswers))\n",
    "            print(\"\\n --------------------------\\n\")\n",
    "        myans,z = trainnet.use(test,retZ=True)\n",
    "        print(\"RMSE IN TEST IS: \")\n",
    "        print(self.error_func_regress(myans,testanswers))\n",
    "        tryans,z = trainnet.use(self.X,retZ=True)\n",
    "        print(self.T.shape)\n",
    "        print(self.X.shape)\n",
    "        if self.X.shape[1] == 1:\n",
    "            plt.figure(figsize=(15,8))\n",
    "            plt.title(\"ACTUAL MULTI DIMENSION ACTUALIZED IN TWO DIMENSIONS\")\n",
    "            plt.scatter(self.X,self.T)\n",
    "            xs, ys = zip(*sorted(zip(self.X, tryans)))\n",
    "\n",
    "            plt.plot(xs, ys,color=\"orange\")\n",
    "            #plt.plot(self.X,tryans,color=\"orange\")\n",
    "            plt.title(\"PREDICTED MULTI DIMENSION ACTUALIZED IN TWO DIMENSIONS\")\n",
    "            plt.figure(figsize=(15,8))\n",
    "            plt.scatter(test,testanswers)  \n",
    "            xs, ys = zip(*sorted(zip(test, myans)))\n",
    "\n",
    "            plt.plot(xs, ys,color=\"orange\")\n",
    "            #plt.plot(test,myans,color=\"orange\")\n",
    "            \n",
    "        else:\n",
    "            #print(self.X[:,0].shape)\n",
    "            plt.figure(figsize=(15,8))\n",
    "            plt.title(\"ACTUAL MULTI DIMENSION ACTUALIZED IN TWO DIMENSIONS\")\n",
    "            plt.scatter(self.X[:,0],self.T)  \n",
    "            xs, ys = zip(*sorted(zip(self.X[:,0], tryans)))\n",
    "\n",
    "            plt.plot(xs, ys,color=\"orange\")\n",
    "            #plt.plot(self.X[:,0],tryans,color=\"orange\")\n",
    "            plt.figure(figsize=(15,8))\n",
    "            plt.title(\"ACTUAL MULTI DIMENSION ACTUALIZED IN TWO DIMENSIONS\")\n",
    "            plt.scatter(test[:,0],testanswers)  \n",
    "            xs, ys = zip(*sorted(zip(test[:,0], myans)))\n",
    "\n",
    "            plt.plot(xs, ys,color=\"orange\")\n",
    "            #plt.plot(test[:,0],myans,color=\"orange\")\n",
    "            #print(self.X[:,2].shape)\n",
    "            plt.figure(figsize=(15,8))\n",
    "            plt.title(\"PREDICTED MULTI DIMENSION ACTUALIZED IN TWO DIMENSIONS\")\n",
    "            plt.scatter(self.X[:,2],self.T)\n",
    "            xs, ys = zip(*sorted(zip(self.X[:,0], tryans)))\n",
    "\n",
    "            plt.plot(xs, ys,color=\"orange\")\n",
    "            #plt.plot(self.X[:,2],tryans,color=\"orange\")\n",
    "            plt.figure(figsize=(15,8))\n",
    "            plt.title(\"PREDICTED MULTI DIMENSION ACTUALIZED IN TWO DIMENSIONS\")\n",
    "            \n",
    "            plt.scatter(test[:,2],testanswers)  \n",
    "            xs, ys = zip(*sorted(zip(test[:,2], myans)))\n",
    "\n",
    "            plt.plot(xs, ys,color=\"orange\")\n",
    "            #plt.plot(test[:,2],myans,color=\"orange\")\n",
    "            #print(self.X[:,2].shape)\n",
    "            plt.figure(figsize=(15,8))\n",
    "            plt.scatter(self.X[:,5],self.T)  \n",
    "            xs, ys = zip(*sorted(zip(self.X[:,0], tryans)))\n",
    "\n",
    "            plt.plot(xs, ys,color=\"orange\")\n",
    "            #plt.plot(self.X[:,5],tryans,color=\"orange\")\n",
    "            plt.figure(figsize=(15,8))\n",
    "            plt.scatter(test[:,5],testanswers)  \n",
    "            xs, ys = zip(*sorted(zip(test[:,5], myans)))\n",
    "\n",
    "            plt.plot(xs, ys,color=\"orange\")\n",
    "            plt.plot(test[:,5],myans,color=\"orange\")\n",
    "            \n",
    "    def five_fold_classify(self,numclasses):\n",
    "        print(self.X.shape)\n",
    "        part = int(self.X.shape[0]/5)\n",
    "        train = self.X[:5*part+1,:]\n",
    "        trainanswers = self.T[:5*part+1,:]\n",
    "        validate = self.X[3*part+1:4*part+1,:]\n",
    "        validateanswers = self.T[3*part+1:4*part+1:,:]\n",
    "        test = self.X[4*part+1:5*part:,:]\n",
    "        testanswers = self.T[4*part+1:5*part:,:]\n",
    "        #trainnet = NeuralNetLog([self.X.shape[1],5,2])\n",
    "        #trainnet.train(self.X,self.T)\n",
    "        #return\n",
    "        #trainnet.train(train[0:9999,:],trainanswers[0:9999,:])\n",
    "        \n",
    "        models = [[self.X.shape[1],25,numclasses],[self.X.shape[1],11,numclasses],[self.X.shape[1],3,numclasses],[self.X.shape[1],10,numclasses],[self.X.shape[1],20,numclasses]]\n",
    "        fold = part\n",
    "        for i in range(0,train.shape[0],fold):\n",
    "            trainnet = NeuralNet(models[int(i/fold)])\n",
    "            rn = random.randint(0,4)\n",
    "            validate = self.X[(rn*part)+1:((rn+1)*part)+1,:]\n",
    "            validateanswers = self.T[(rn*part)+1:((rn+1)*part)+1,:]\n",
    "            if train.shape[0] < i + fold -1:\n",
    "                trainnet.train(train[i:i+train.shape[0]-1,:],trainanswers[i:i+train.shape[0]-1,:],ftracep=True)\n",
    "                \n",
    "            else:\n",
    "                trainnet.train(train[i:i+fold-1,:],trainanswers[i:i+fold-1,:],ftracep=True)\n",
    "            \n",
    "            myans,z = trainnet.use(validate,retZ=True)\n",
    "            print(\"\\n -------------------------- \\n\")\n",
    "            print(\"ACCURACY IN PART \" + str(int(i/fold) + 1) + \" IS: \")\n",
    "            print(self.error_func_classify(myans,validateanswers))\n",
    "            print(\"\\n --------------------------\\n\")\n",
    "        myans,z = trainnet.use(test,retZ=True)\n",
    "        print(\"ACCURACY IN TEST IS: \")\n",
    "        print(self.error_func_classify(myans,testanswers))\n",
    "        tryans,z = trainnet.use(self.X,retZ=True)\n",
    "        print(self.T.shape)\n",
    "        print(self.X.shape)\n",
    "        #print(self.X[:,0].shape)\n",
    "        #Xans = np.append(test,myans,1)\n",
    "        Xans = np.append(test,myans.argmax(axis=1).reshape(len(myans),1),1)\n",
    "        Xtrain = np.append(test,testanswers,1)\n",
    "\n",
    "        print(X)\n",
    "        print(X.shape)\n",
    "        n = np.unique(Xtrain[:,X.shape[1]])\n",
    "        groupednp = np.array( [ list(Xtrain[Xtrain[:,X.shape[1]]==i,1]) for i in n])\n",
    "        groupednp2 = np.array( [ list(Xtrain[Xtrain[:,X.shape[1]]==i,2]) for i in n])\n",
    "\n",
    "        n = np.unique(Xans[:,X.shape[1]])\n",
    "        groupednpans = np.array( [ list(Xans[Xans[:,X.shape[1]]==i,1]) for i in n])\n",
    "        groupednpans2 = np.array( [ list(Xans[Xans[:,X.shape[1]]==i,2]) for i in n])\n",
    "        \n",
    "        fig = plt.figure(figsize=(15,8))\n",
    "        ax1 = fig.add_subplot(111)\n",
    "        plt.title(\"ACTUAL MULTI DIMENSION ACTUALIZED IN TWO DIMENSIONS\")\n",
    "        #color = colors[T.argmax(0)]\n",
    "        for i in range(len(groupednp)):\n",
    "            groupednp[i] = (np.array(groupednp[i]) - np.array(groupednp[i]).min())/(np.array(groupednp[i]).max() - np.array(groupednp[i]).min())\n",
    "            groupednp2[i] = (np.array(groupednp2[i]) - np.array(groupednp2[i]).min())/(np.array(groupednp2[i]).max() - np.array(groupednp2[i]).min())\n",
    "            ax1.scatter(groupednp[i],groupednp2[i])\n",
    "    \n",
    "        fig = plt.figure(figsize=(15,8))\n",
    "        ax1 = fig.add_subplot(111)\n",
    "        plt.title(\"PREDICTED MULTI DIMENSION ACTUALIZED IN TWO DIMENSIONS\")\n",
    "        #color = colors[T.argmax(0)]\n",
    "        for i in range(len(groupednpans)):\n",
    "            groupednpans[i] = (np.array(groupednpans[i]) - np.array(groupednpans[i]).min())/(np.array(groupednpans[i]).max() - np.array(groupednpans[i]).min())\n",
    "            groupednpans2[i] = (np.array(groupednpans2[i]) - np.array(groupednpans2[i]).min())/(np.array(groupednpans2[i]).max() - np.array(groupednpans2[i]).min())\n",
    "    \n",
    "            ax1.scatter(groupednpans[i],groupednpans2[i])\n",
    "        \n",
    "        \"\"\"\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        plt.figure(figsize=(15,8))\n",
    "        plt.scatter(self.X[:,0],self.T)  \n",
    "        plt.plot(self.X[:,0],tryans,color=\"orange\")\n",
    "        plt.figure(figsize=(15,8))\n",
    "        plt.scatter(test[:,0],testanswers)  \n",
    "        plt.plot(test[:,0],myans,color=\"orange\")\n",
    "        #print(self.X[:,2].shape)\n",
    "        plt.figure(figsize=(15,8))\n",
    "        plt.scatter(self.X[:,2],self.T)  \n",
    "        plt.plot(self.X[:,2],tryans,color=\"orange\")\n",
    "        plt.figure(figsize=(15,8))\n",
    "        plt.scatter(test[:,2],testanswers)  \n",
    "        plt.plot(test[:,2],myans,color=\"orange\")\n",
    "        #print(self.X[:,2].shape)\n",
    "        plt.figure(figsize=(15,8))\n",
    "        plt.scatter(self.X[:,5],self.T)  \n",
    "        plt.plot(self.X[:,5],tryans,color=\"orange\")\n",
    "        plt.figure(figsize=(15,8))\n",
    "        plt.scatter(test[:,5],testanswers)  \n",
    "        plt.plot(test[:,5],myans,color=\"orange\")\n",
    "        \"\"\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scipy.stats import itemfreq\n",
    "#np.asmatrix(humid).shape[0]\n",
    "#np.asmatrix(temper).shape[0]\n",
    "#copy.copy([1,2,3])\n",
    "print(df.values.shape)\n",
    "groupwithout.ix[:,[2,3,4]]\n",
    "#groupwithout.ix[:,[0:3]]\n",
    "X = df.ix[:,[1,3,4,5,6,7,8,9,10]].values\n",
    "X = X[:,:]\n",
    "T = df.ix[:,[2]].values\n",
    "\n",
    "T = T[:,:]\n",
    "print(type(T))\n",
    "\n",
    "\n",
    "classes = np.unique(T)\n",
    "numclasses = len(np.unique(T))\n",
    "print(\"numclasses\")\n",
    "print(numclasses)\n",
    "base_class = np.array([0 for i in range(numclasses)])\n",
    "\n",
    "new_T = np.array([base_class]*T.shape[0])\n",
    "\n",
    "print(new_T)\n",
    "print(new_T.shape)\n",
    "\n",
    "for i in range(X.shape[0]):\n",
    "    #print(T[i])\n",
    "    #print(np.where(classes==T[i]))\n",
    "    new_T[i][np.where(classes==T[i])] = 1\n",
    "    \n",
    "\n",
    " \n",
    "\n",
    "print(new_T)\n",
    "print(X.shape)\n",
    "print(new_T.shape)\n",
    "\n",
    "\n",
    "#trainnet = NeuralNetLog([X.shape[1],20,numclasses])\n",
    "#trainnet.train(X,new_T,ftracep=True)\n",
    "cc = CrossValid(X,new_T)\n",
    "cc.five_fold_classify(numclasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import itemfreq\n",
    "#np.asmatrix(humid).shape[0]\n",
    "#np.asmatrix(temper).shape[0]\n",
    "#copy.copy([1,2,3])\n",
    "print(df.values.shape)\n",
    "groupwithout.ix[:,[2,3,4]]\n",
    "#groupwithout.ix[:,[0:3]]\n",
    "X = df.ix[:,[3,4,5,7,10,11,12,13,14,15,16,17,18,19,21]].values\n",
    "X = X[:35000,:]\n",
    "T = df.ix[:,[20]].values\n",
    "\n",
    "T = T[:35000,:]\n",
    "print(type(T))\n",
    "\n",
    "\n",
    "classes = np.unique(T)\n",
    "numclasses = len(np.unique(T))\n",
    "\n",
    "base_class = np.array([0 for i in range(numclasses)])\n",
    "\n",
    "new_T = np.array([base_class]*T.shape[0])\n",
    "\n",
    "print(new_T)\n",
    "print(new_T.shape)\n",
    "\n",
    "for i in range(X.shape[0]):\n",
    "    #print(T[i])\n",
    "    #print(np.where(classes==T[i]))\n",
    "    new_T[i][np.where(classes==T[i])] = 1\n",
    "    \n",
    "\n",
    "#trainnet = NeuralNetLog([X.shape[1],20,numclasses])\n",
    "#trainnet.train(X,new_T,ftracep=True)\n",
    " \n",
    "\n",
    "print(new_T)\n",
    "print(new_T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Done\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = CrossValid(X,new_T)\n",
    "cc.five_fold_classify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainnet = NeuralNet([1,3,1])\n",
    "humid = humid.reshape(humid.shape[0],1)\n",
    "\n",
    "temper = temper.reshape(temper.shape[0],1)\n",
    "print(temper.shape)\n",
    "print(humid.shape)\n",
    "result = trainnet.train(humid, temper, ftracep=True)\n",
    "#print(result[\"ftrace\"])\n",
    "Ytest, Z = trainnet.use(humid, retZ=True)\n",
    "\n",
    "#print(Ytest)\n",
    "#print(\"done\")\n",
    "humid.shape\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.title(\"PREDICTED MULTI DIMENSION ACTUALIZED IN TWO DIMENSIONS\")\n",
    "plt.scatter(humid,temper)  \n",
    "plt.plot(humid,Ytest,color=\"orange\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result = trainnet.train(humid, temper, ftracep=True)\n",
    "#print(result[\"ftrace\"])\n",
    "#Ytest, Z = trainnet.use(humid, retZ=True)\n",
    "#print(temper.shape)\n",
    "\n",
    "cc = CrossValid(humid,temper)\n",
    "cc.five_fold_regress()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data NORMALIZATION\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mpl_toolkits import mplot3d\n",
    "import copy\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"ausweather_preprocessed.csv\",sep=\"\\t\")\n",
    "\n",
    "\n",
    "grouped_by_month_rainfall = df.groupby([df.Date.str[:11],\"Location\"])[\"Location\",\"MaxTemp\",\"Humidity9am\"].mean().reset_index()\n",
    "\n",
    "#print grouped_by_month_rainfall\n",
    "grouped_2013 = grouped_by_month_rainfall\n",
    "grouped_2013 = grouped_by_month_rainfall[ (grouped_2013[\"Location\"]==\"Katherine\")  | (grouped_2013[\"Location\"]==\"Bendigo\") ]\n",
    "\n",
    "#print grouped_2013\n",
    "\n",
    "\n",
    "\n",
    "#plt.figure(figsize=(15,8))\n",
    "#plt.title(\"Humidity vs Max Temperature of Bendigo\")\n",
    "#plt.xlabel('Humidity', foanalysis / comparison of algorithmsntsize=18)\n",
    "#plt.ylabel('Max Temperature', fontsize=16)\n",
    "#EXAMPLE OF A SIMPLE TWO DIVISION CLUSTER. DATA FROM EITHER ONE OF THE SOURCES CAN BE CLASSIFIED TO EITHER ONE BASED ON SOME CLASSIFICATION ALGORITHM\n",
    "k = 0\n",
    "\n",
    "for name,group in grouped_2013.groupby(\"Location\"):\n",
    "    if(k>0):#analysis  comparison of algorithms\n",
    "        break\n",
    "    humidity_points = group[\"Humidity9am\"]\n",
    "    maxtemp_points = group[\"MaxTemp\"]\n",
    "    #plt.scatter(group[\"Humidity9am\"],group[\"MaxTemp\"])\n",
    "    k+=1\n",
    "\n",
    "\n",
    "humid = np.array(df[\"Humidity9am\"])\n",
    "temper = np.array(df[\"MaxTemp\"])\n",
    "\n",
    "humid_normal = humid\n",
    "temper_normal = temper\n",
    "\n",
    "humid_max = humid.max()\n",
    "humid_min = humid.min()\n",
    "humid_avg = humid.mean()\n",
    "\n",
    "temper_max = temper.max()\n",
    "temper_min = temper.min()\n",
    "temper_avg = temper.mean()\n",
    "\n",
    "\n",
    "for i in range(len(humid)):\n",
    "    humid[i] = (humid[i] - humid_avg)/(humid_max - humid_min)\n",
    "    \n",
    "for i in range(len(temper)):\n",
    "    temper[i] = (temper[i] - temper_avg)/(temper_max - temper_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"done\")\n",
    "humid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainnet = NeuralNet([1,1,1])\n",
    "humid = humid.reshape(humid.shape[0],1)\n",
    "\n",
    "temper = temper.reshape(temper.shape[0],1)\n",
    "print(temper.shape)\n",
    "print(humid.shape)\n",
    "result = trainnet.train(humid, temper, ftracep=True)\n",
    "print(result[\"ftrace\"])\n",
    "Ytest, Z = trainnet.use(humid, retZ=True)\n",
    "\n",
    "print(Ytest)\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.scatter(humid,temper) \n",
    "xs, ys = zip(*sorted(zip(humid, Ytest)))\n",
    "\n",
    "plt.plot(xs, ys,color=\"orange\")\n",
    "#plt.plot(humid,Ytest,color=\"orange\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = CrossValid(humid,temper)\n",
    "cc.five_fold_regress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.asmatrix(humid).shape[0]\n",
    "#np.asmatrix(temper).shape[0]\n",
    "#copy.copy([1,2,3])\n",
    "print(df.values.shape)\n",
    "groupwithout.ix[:,[2,3,4]]\n",
    "#groupwithout.ix[:,[0:3]]\n",
    "X = df.ix[:,[3,4,7,10,11,12,13,14,15,16,17,18,19,21]].values\n",
    "\n",
    "T = df.ix[:,[5]].values\n",
    "\n",
    "X_max = X.max(axis=0)\n",
    "X_min = X.min(axis=0)\n",
    "X_avg = X.mean(axis=0)\n",
    "\n",
    "T_max = T.max()\n",
    "T_min = T.min()\n",
    "T_avg = T.mean()\n",
    "\n",
    "\n",
    "print(X_max.shape)\n",
    "for i in range(X.shape[0]):\n",
    "    #print(X[i,:].shape)\n",
    "    #print(X_avg.shape)\n",
    "    #print(X_min.shape)\n",
    "    #print(X_max.shape)\n",
    "    X[i,:] = (X[i,:] - X_avg)/(X_max - X_min)\n",
    "print(X.shape)\n",
    "    \n",
    "for i in range(len(T)):\n",
    "    T[i] = (T[i] - T_avg)/(T_max - T_min)\n",
    "    \n",
    "print(X.shape)\n",
    "print(T.shape)\n",
    "#groupwithout[\"MaxTemp\"],groupwithout[\"Humidity9am\"],groupwithout[\"Rainfall\"]\n",
    "#trainnet = NeuralNet([X.shape[1],10,1])\n",
    "#trainnet.train(X,T,ftracep=True)\n",
    "\n",
    "\n",
    "#answers,z = trainnet.use(X, retZ=True)\n",
    "cc = CrossValid(X,T)\n",
    "cc.five_fold_regress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainnet = NeuralNet([X.shape[1],3,1])\n",
    "trainnet.train(X,T,ftracep=True)\n",
    "\n",
    "\n",
    "answers,z = trainnet.use(X, retZ=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Done\")\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "print(X[:,0].shape)\n",
    "plt.scatter(X[:,7],T)\n",
    "\n",
    "xs, ys = zip(*sorted(zip(X[:,7], answers)))\n",
    "\n",
    "plt.plot(xs, ys,color=\"orange\")\n",
    "#plt.plot(X[:,7],answers,color=\"orange\")\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "print(X[:,0].shape)\n",
    "plt.scatter(X[:,1],T)\n",
    "\n",
    "xs, ys = zip(*sorted(zip(X[:,1], answers)))\n",
    "\n",
    "plt.plot(xs, ys,color=\"orange\")\n",
    "#plt.plot(X[:,1],answers,color=\"orange\")\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "print(X[:,0].shape)\n",
    "plt.scatter(X[:,2],T)\n",
    "\n",
    "xs, ys = zip(*sorted(zip(X[:,2], answers)))\n",
    "\n",
    "plt.plot(xs, ys,color=\"orange\")\n",
    "#plt.plot(X[:,2],answers,color=\"orange\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainnet = NeuralNet([1,2,1])\n",
    "humid = humid.reshape(humid.shape[0],1)\n",
    "\n",
    "temper = temper.reshape(temper.shape[0],1)\n",
    "print(humid.shape)\n",
    "print(temper.shape)\n",
    "cc = CrossValid(humid,temper)\n",
    "cc.five_fold_regress()\n",
    "\n",
    "trainnet.train(humid,temper,ftracep=True)\n",
    "myans,z = trainnet.use(humid,retZ=True)\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.scatter(humid,temper)  \n",
    "plt.plot(humid,myans,color=\"orange\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.asmatrix(humid).shape[0]\n",
    "#np.asmatrix(temper).shape[0]\n",
    "#copy.copy([1,2,3])\n",
    "groupwithout.ix[:,[2,3,4]]\n",
    "#groupwithout.ix[:,[0:3]]\n",
    "X = groupwithout.ix[:,[2,3]].values\n",
    "T = groupwithout.ix[:,[4]].values\n",
    "print(X.shape)\n",
    "print(T.shape)\n",
    "#groupwithout[\"MaxTemp\"],groupwithout[\"Humidity9am\"],groupwithout[\"Rainfall\"]\n",
    "trainnet = NeuralNet([X.shape[1],2,1])\n",
    "trainnet.train(X,T,ftracep=True)\n",
    "answers,z = trainnet.use(X, retZ=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainnet = NeuralNet([1,20,1])\n",
    "humid = humid.reshape(humid.shape[0],1)\n",
    "\n",
    "temper = temper.reshape(temper.shape[0],1)\n",
    "#print(temper.shape)\n",
    "#print(humid.shape)\n",
    "result = trainnet.train(humid, temper, ftracep=True)\n",
    "#print(result[\"ftrace\"])\n",
    "Ytest, Z = trainnet.use(humid, retZ=True)\n",
    "\n",
    "#print(Ytest)\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.scatter(humid,temper)  \n",
    "plt.plot(humid,Ytest,color=\"orange\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter/network structure choice\n",
    "\n",
    "The network choice has been different, and so has the choice of hyperparameters. In order to define as to what exactly was chosen, we take into consideration, let us list the hyperparameters:\n",
    "\n",
    "- Learning Rate\n",
    "- Number of iterations\n",
    "- Lambda (Regularization penalty factor)\n",
    "- Number of hidden layer neurons\n",
    "- Number of output layer neurons\n",
    "\n",
    "This being a neural network assignment rather than a deep learning assignment the choice was to use a single hidden layer , and play with the number of neurons in it.So in each case, we choose models such that , the number of hidden layer neurons is less than the number of input features in GENERAL, and the output is less than the number of hidden layer neurons.\n",
    "\n",
    "In each case we create a certain number of neurons in a model which we use in the cross validation. Our core idea behind using a network structure is to pre-initialize it. \n",
    "\n",
    "For the regression ,we pick different hidden layer neurons - 7,3,5,11. Each of these activate a tanh, in order to give a complexity to the equation. The right amount of complexity is necessary to fit the training data. Thus we can notice that the best way to do so is to use one of the above mentioned models.All of these map to one Neuron where the output is not passed through an activation, and is a direct result of the weights times parameters with the bias.\n",
    "\n",
    "For the classification ,we pick different hidden layer neurons - 7,3,5,11. Each of these activate a tanh, in order to give a complexity to the equation. The right amount of complexity is necessary to fit the training data. Thus we can notice that the best way to do so is to use one of the above mentioned models.All of these map to one Neuron where the output is passed through an activation, and is a direct result of the weights times parameters with the bias.The output is passed through either sigmoid activation or softmax to give an output which would classify the data into binary or multi class respectively.\n",
    "\n",
    "We pick a higher than 0 Lambda for classification, since we don't want to heavily overfit the classes, and rather want to fit the case. We can see that this serves the purpose. \n",
    "\n",
    "We use a regular learning rate without much change, and only one hidden layer with different possible neurons in each. We use 1000 iterations in each for each of these cases.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDICTION RESULTS\n",
    "\n",
    "We have to consider both the following algorithms we implemented:\n",
    "    - For Non Linear Regression we have to consider that we used a large data set, which is spread across a large region, and is technically not easy to fit. In the above examples we are given to see that our model does indeed fit the data well, with the right amount of neurons in the hidden layer. \n",
    "    - We need to consider that we didn't allow any regularization for this. This helped us, in that we could visualize a very high order polynomial in this experiment very easily. That was the goal of our experiment which we achieved with relative ease.\n",
    "    - For Non Linear Logistic Regression we achieved an accuracy of even:\n",
    "        - 97% to 99% in some cases\n",
    "    - This was possible even in the test data, and not just training because of the cross validation and that the neural network was trained well with the information it was provided and with good hyperparameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "We got to learn the base of how the neural net was actually trained and used. It gives us an intuition of how we should use raw information and classify or perform regression, and achieve any polynomial by just using one hidden layer of neurons. This also gives us an intuition of how we can use the regularization and learning rate to get the desired fit to the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Credit\n",
    "\n",
    "Now you are testing various **activation functions** in this section. Use the best neural network structure and explore 3 different activation functions of your choice (one should be *tanh* that you used in the previous sections). \n",
    "You should use cross validation to discover the best model (with activation function). \n",
    "\n",
    "\n",
    "One extra credit is assigned when you finish the work completely. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignmentvenv",
   "language": "python",
   "name": "envname"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
