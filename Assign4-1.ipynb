{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment #4 - Reinforcement Learning\n",
    "\n",
    "### <font color=\"red\"> DUE: Apr 16 (Tuesday) 11:00 pm </font>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\"> Anirudh Narayanan</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Overview\n",
    "\n",
    "The key goal of this assignment is to learn, use and evaluate reinforcement learning on a famous card game (blackjack), with it's different flavors (SARSA , Q Learning). Reinforcement learning is one where, we provide positive and negetive feedback using a pre-defined reward based idea. The key goal when using reinforcement learning for a problem like blackjack, is to define for each of hit or stand whether we choose to do either one based on what we have learned from the past, and a pre-defined reward system. We find out how the hyperparameters of alpha epsilon and theta function together in unison to tune the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Problems \n",
    "\n",
    "\n",
    "\n",
    "## Blackjack\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/a/a2/Blackjack_game_1.JPG\" width=800 />\n",
    "\n",
    "Now, we play Blackjack! \n",
    "We have improved version of it from OpenAI Gym [Blackjack-V0](https://gym.openai.com/envs/Blackjack-v0/). Our blackjack has an additional betting option. Here follows the rule description. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### <font color=\"red\"> Game Introduction and Rules: </font>\n",
    "\n",
    "\n",
    "Blackjack is a card game where the goal is to obtain cards that sum to as\n",
    "near as possible to 21 without going over.  They're playing against a fixed\n",
    "dealer.\n",
    "\n",
    "Face cards (Jack, Queen, King) have point value 10.\n",
    "Aces can either count as 11 or 1, and it's called 'usable' at 11.\n",
    "This game is placed with an infinite deck (or with replacement).\n",
    "\n",
    "The game starts with each player getting two face up cards and dealer\n",
    "having one face up and face down card and the player needs to bet an \n",
    "amount of money within range of 1 to max bet (10). \n",
    "Note: Face up means that you will know what the card is.\n",
    "\n",
    "The player can request additional cards (hit=1) until they decide to stop\n",
    "(stick=0) or exceed 21 (bust).\n",
    "\n",
    "After the player sticks, the dealer reveals their facedown card, and draws\n",
    "until their sum is 17 or greater (fixed policy).  If the dealer goes bust the player wins.\n",
    "You won't see the dealer's facedown card, just you will know if you have or lost\n",
    "and you will get appropriate reward of +1 if you win, -1 if you loose or 0 if the match\n",
    "is drawn.\n",
    "(It is freedom of choice for you to change the reward function. If you want to change, you should explain the reason of the change.)\n",
    "\n",
    "If the player wins, the amount he has bet will be doubled and given back. \n",
    "If the player and dealer have the same sum, then its a draw and the player will get\n",
    "back the money he has bet. \n",
    "If the player loses then, the money he bet will be lost. \n",
    "If neither player nor dealer busts, the outcome (win, lose, draw) is\n",
    "decided by whose sum is closer to 21. You are free to change the reward function to make it \n",
    "learn more efficiently. (i.e.,  the amount of money the user is winning after each round). \n",
    "\n",
    "Regardless of the number of players on the table, each player will be just playing against the \n",
    "dealer independently of the other players."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BlackJack Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Player class definition\n",
    "\n",
    "Player class defines an object for each BlackJack player with a given *unique* name. It has three attributes, the player's name, current cards in hand, and total balance left to play games.  <code>get_info()</code> is getter method that prints out all three attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player():\n",
    "    \"\"\"\n",
    "        \n",
    "        Player class to create a player object.\n",
    "        eg: player = Player(\"player1\", start_balance = 10)\n",
    "        Above declaration will be for your agent.\n",
    "        All the player names should be unique or else you will get error.\n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self, player_name, start_balance = 10):\n",
    "        self.player_name = player_name\n",
    "        self.card = []\n",
    "        self.starter_balance = start_balance\n",
    "        self.total_balance = start_balance\n",
    "\n",
    "    def recharge_account(self):\n",
    "        self.total_balance = self.starter_balance\n",
    "        \n",
    "    def get_info(self):\n",
    "        print(\"Player name: {}\".format(self.player_name))\n",
    "        print(\"Player card: {}\".format(self.card))\n",
    "        print(\"Player total_balance: {}\".format(self.total_balance))     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following functions are utilities and definition of card deck to play games and to check the game status. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 = Ace, 2-10 = Number cards, Jack/Queen/King = 10\n",
    "deck = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 10, 10]\n",
    "\n",
    "def cmp(a, b):\n",
    "    return float(a > b) - float(a < b)\n",
    "\n",
    "def draw_card(np_random):\n",
    "    return int(np_random.choice(deck))\n",
    "\n",
    "def draw_hand(np_random):\n",
    "    return [draw_card(np_random), draw_card(np_random)]\n",
    "\n",
    "\n",
    "def usable_ace(hand):  # Does this hand have a usable ace?\n",
    "    return 1 in hand and sum(hand) + 10 <= 21\n",
    "\n",
    "\n",
    "def sum_hand(hand):  # Return current hand total\n",
    "    if usable_ace(hand):\n",
    "        return sum(hand) + 10\n",
    "    return sum(hand)\n",
    "\n",
    "\n",
    "def is_bust(hand):  # Is this hand a bust?\n",
    "    return sum_hand(hand) > 21\n",
    "\n",
    "\n",
    "def score(hand):  # What is the score of this hand (0 if bust)\n",
    "    return 0 if is_bust(hand) else sum_hand(hand)\n",
    "\n",
    "\n",
    "def is_natural(hand):  # Is this hand a natural blackjack?\n",
    "    return sorted(hand) == [1, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Game Environment\n",
    "\n",
    "The Blackjack game environment is defined as follows. Inheriting, OpenAI.Gym.Env, it overrides the interfances to enable the interfaction with the same interfaces with other OpenAI Gym environments. \n",
    "\n",
    "These are important member functions to be used by you:\n",
    "1. add_player: You can add your player object with some initial amount of money\n",
    "\n",
    "2. step: you need to send the action using this function, actions are hit(1) or stand(0)\n",
    "        returns {'state': player info and dealer info (_get_obs), \n",
    "                 'reward': real-valued reward, \n",
    "                 'is_round_done': True/Fasle}\n",
    "3. _get_obs: returns the state of the env or required information you can use to build your agent. \n",
    "             eg: {'player_info':{'player_sum_card': sum of cards, \n",
    "                                 'player_card': list of cards, \n",
    "                                 'player_total_balance': real-valued number, \n",
    "                                 'usable_ace' : True/False}, \n",
    "                 'dealer_info': dealer's first card}\n",
    "4. init_round: will start a new round in the same game.\n",
    "        returns {'state': player info and dealer info (_get_obs)}\n",
    "          \n",
    "5. reset: will reset the whole game and you will being again with the initial balance you had started the player object.\n",
    "        returns nothing, will just reset the entire game.\n",
    "          \n",
    "6. bet_money: you can bet the amount using this method, takes in the money between (1, max_bet_cap). max_bet_cap is 10. \n",
    "        returns nothing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review of SARSA and Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q Learning \n",
    "Q-learning is a model-free reinforcement learning algorithm. The goal of Q-learning is to learn a policy, which tells an agent what action to take under what circumstances. It does not require a model and it can handle problems with stochastic transitions and rewards, without requiring adaptations. Q-Learnings core is based on using the Q-Table. The following are decided when it comes to reinforcement learning:\n",
    "<br>\n",
    "\n",
    "- Actions\n",
    "- States\n",
    "- Rewards\n",
    "- Policy\n",
    "- Values\n",
    "\n",
    "The key idea behind using Q-Learning is policy control. There are 2 parts to any RL problem. Exploitation and Exploration. If the algorithm merely exploits the known information to make a choice it is known as a greedy one, and in Q-Learning the Q-Table solves that, as it leverages the table which is constantly updated, and thereby makes decisions. Thus Q-Learning (Reinforcement).\n",
    "\n",
    "## SARSA\n",
    "State–action–reward–state–action (SARSA) is an algorithm for learning a Markov decision process policy, used in the reinforcement learning area of machine learning. \n",
    "\n",
    "### Alpha\n",
    "The learning rate determines to what extent newly acquired information overrides old information. A factor of 0 will make the agent not learn anything, while a factor of 1 would make the agent consider only the most recent information. (alpha)\n",
    "\n",
    "### Discount Factor Gamma\n",
    "The discount factor determines the importance of future rewards. A factor of 0 makes the agent \"opportunistic\" by only considering current rewards, while a factor approaching 1 will make it strive for a long-term high reward. If the discount factor meets or exceeds 1, the Q values may diverge.\n",
    "\n",
    "### Initial Conditions\n",
    "\n",
    "A low (infinite) initial value, also known as \"optimistic initial conditions\", can encourage exploration: no matter what action takes place, the update rule causes it to have higher values than the other alternative, thus increasing their choice probability. In 2013 it was suggested that the first reward r could be used to reset the initial conditions.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class BlackjackEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Simple blackjack environment\n",
    "    \n",
    "    Blackjack is a card game where the goal is to obtain cards that sum to as\n",
    "    near as possible to 21 without going over.  They're playing against a fixed\n",
    "    dealer.\n",
    "    Face cards (Jack, Queen, King) have point value 10.\n",
    "    Aces can either count as 11 or 1, and it's called 'usable' at 11.\n",
    "    This game is placed with an infinite deck (or with replacement).\n",
    "    The game starts with each (player and dealer) having one face up and one\n",
    "    face down card.\n",
    "    The player can request additional cards (hit=1) until they decide to stop\n",
    "    (stick=0) or exceed 21 (bust).\n",
    "    After the player sticks, the dealer reveals their facedown card, and draws\n",
    "    until their sum is 17 or greater.  If the dealer goes bust the player wins.\n",
    "    If neither player nor dealer busts, the outcome (win, lose, draw) is\n",
    "    decided by whose sum is closer to 21.  The reward for winning is +1,\n",
    "    drawing is 0, and losing is -1.\n",
    "    The observation is: the players current sum, players current card, players balance left, \n",
    "    the dealer's one showing card (1-10 where 1 is ace),\n",
    "    and whether or not the player holds a usable ace (0 or 1).\n",
    "    \n",
    "    Parameters\n",
    "    ==========\n",
    "    natural      boolean\n",
    "                 option for 50% more reward on natural blackjack \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, natural=False):\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        self.observation_space = spaces.Tuple((\n",
    "            spaces.Discrete(32),\n",
    "            spaces.Discrete(11),\n",
    "            spaces.Discrete(2)))\n",
    "        self.seed()\n",
    "        self.total_players = 1\n",
    "        self.players = {}\n",
    "        self.natural = natural\n",
    "        self.dealer = []\n",
    "        self.round_bet_info = {}\n",
    "        self.max_bet_cap = 10\n",
    "\n",
    "    def add_player(self, player_class):\n",
    "        if player_class not in self.players and len(self.players) < self.total_players:\n",
    "            self.players[player_class.player_name] = player_class\n",
    "        else:\n",
    "            raise Exception(\"Maximum number of player allowed: {}. You can increase the player count while initializing the environment\".format(len(self.players)))\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def step(self, player_name, action):\n",
    "        assert self.action_space.contains(action)\n",
    "        if action:  # hit: add a card to players hand and return\n",
    "            self.players[player_name].card.append(draw_card(self.np_random))\n",
    "            if is_bust(self.players[player_name].card):\n",
    "                done = True\n",
    "                reward = -1\n",
    "            else:\n",
    "                done = False\n",
    "                reward = 0\n",
    "        else:  # stick: play out the dealers hand, and score\n",
    "            done = True\n",
    "            while sum_hand(self.dealer) < 17:\n",
    "                self.dealer.append(draw_card(self.np_random))\n",
    "            reward = cmp(score(self.players[player_name].card), score(self.dealer))\n",
    "            if self.natural and is_natural(self.players[player_name].card) and reward == 1:\n",
    "                reward = 1.5\n",
    "        if done:\n",
    "            self.settle_balance(int(reward), player_name)\n",
    "        return {'state':self._get_obs(player_name), 'reward': reward, 'is_round_done': done}\n",
    "\n",
    "    def _get_obs(self, player_name):\n",
    "        return {'player_info':self.get_player_obs(player_name), \n",
    "                'dealer_info': self.dealer[0]}\n",
    "\n",
    "    def get_player_obs(self, player_name):\n",
    "        return {'player_sum_card':sum_hand(self.players[player_name].card), \n",
    "                'player_card':self.players[player_name].card, \n",
    "                'player_total_balance':self.players[player_name].total_balance, \n",
    "                'usable_ace' : usable_ace(self.players[player_name].card)}\n",
    "    \n",
    "    def get_valid_bet_amount(self, player_name):\n",
    "        print(\"player name: {} money left: {}\".format(player_name, self.players[player_name].total_balance))\n",
    "        if self.players[player_name].total_balance < 1:\n",
    "            return {'is_round_done': True, 'valid_bet_amount': -1}\n",
    "        max_bet_upper_limit = self.players[player_name].total_balance\n",
    "        if self.players[player_name].total_balance >= self.max_bet_cap:\n",
    "            max_bet_upper_limit = self.max_bet_cap            \n",
    "        return {'is_round_done': False, 'valid_bet_amount': list(range(1, max_bet_upper_limit+1))}\n",
    "            \n",
    "    def init_round(self, player_name):         \n",
    "        self.dealer = draw_hand(self.np_random)\n",
    "        self.players[player_name].card = draw_hand(self.np_random)\n",
    "        return {'state': self._get_obs(player_name)}        \n",
    "\n",
    "    def bet_money(self, player_name, bet_amount):\n",
    "        self.players[player_name].total_balance -= bet_amount\n",
    "        self.round_bet_info[player_name] = bet_amount\n",
    "            \n",
    "    def settle_balance(self, is_winner, player_name):\n",
    "        if is_winner == 1:\n",
    "            print(\"player winner\")\n",
    "            self.players[player_name].total_balance += (2 * self.round_bet_info[player_name])\n",
    "        elif is_winner == 0:\n",
    "            print(\"Draw match\")\n",
    "            self.players[player_name].total_balance += self.round_bet_info[player_name]\n",
    "        else:\n",
    "            print(\"player loser, wont get back the money, try next round\")\n",
    "    \n",
    "    def reset(self, natural=False):\n",
    "        for player_name, player_class in self.players.items():\n",
    "            player_class.recharge_account()\n",
    "        self.natural = natural\n",
    "        self.dealer = []\n",
    "        self.round_bet_info = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Example Run with Random Players\n",
    "\n",
    "This following code shows how to create an environment and players to play 10 games with 20 maximum rounds. This only includes random betting and random hit/stick decision making. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BlackjackEnv()\n",
    "player1 = Player('player1')\n",
    "env.add_player(player1)\n",
    "#Number of games\n",
    "number_of_games = 10\n",
    "\n",
    "#Number of rounds\n",
    "number_of_rounds = 20\n",
    "\n",
    "for _game in range(number_of_games):\n",
    "\n",
    "    env.reset()\n",
    "    print(\"-\"* 50)\n",
    "    print(\"Game Number: {}\".format(_game + 1))\n",
    "    print(\"-\"* 50)\n",
    "    for _round in range(number_of_rounds): \n",
    "        print(\"%\"* 50)\n",
    "        print(\"Game Number: {} Round Number: {}\".format(_game+1, _round+1))\n",
    "        print(\"%\"* 50)\n",
    "        \n",
    "        #Check if you have enough money left in the account to bet, if not break and start a new game\n",
    "        valid_bet_flag = env.get_valid_bet_amount(player1.player_name)\n",
    "        if valid_bet_flag['is_round_done']:\n",
    "            print(\"You are out of money ! Will go to next game !!\")\n",
    "            break\n",
    "        bet_amount = valid_bet_flag['valid_bet_amount']\n",
    "        \n",
    "        #Get the observations i.e state and use it to decide how much you want to bet\n",
    "        round_obs = env.init_round(player1.player_name)\n",
    "        print(\"Initial state: {}\".format(round_obs))  \n",
    "        \n",
    "        # If you have just $1 left in the bank, you have no choice but to bet that amount \n",
    "        # and hopefully leave it to the fate for your agent to win and continue playing or just\n",
    "        # start a new game\n",
    "        if len(bet_amount) == 1 and bet_amount[0] == 1:\n",
    "            print(\"You can only bet 1. So betting only 1.\")\n",
    "            random_bet = 1\n",
    "        else:\n",
    "            random_bet = np.random.choice(bet_amount)\n",
    "        print(\"Random Init Bet: {}\".format(random_bet))\n",
    "        env.bet_money(player1.player_name, random_bet)\n",
    "          \n",
    "        \n",
    "        # Look at the state and take actions, here the actions are selected randomly\n",
    "        # you can use the agent to select the action\n",
    "        random_action = np.random.randint(0, 2)\n",
    "        print(\"Action taken: {}\".format(random_action))\n",
    "\n",
    "        #Use the selected action to actually take the action in env by calling step\n",
    "        round_obs = env.step(player1.player_name, random_action)\n",
    "        print(\"State after 1st action: {}\".format(round_obs))\n",
    "\n",
    "        #Check if the game is not over, if your action is stand then the game is over, else\n",
    "        #the game continues in this loop until your action is stand\n",
    "        while not round_obs['is_round_done']:\n",
    "            #Select an random action and take the action\n",
    "            random_action = np.random.randint(0, 2)\n",
    "            print(\"Action taken: {}\".format(random_action))\n",
    "            round_obs = env.step(player1.player_name, random_action)\n",
    "            print(\"State after nth action: {}\".format(round_obs))\n",
    "\n",
    "        #Round over, but the game will continue untill you have exhausted your initial money \n",
    "        print(\"Balance Left: {}\".format(round_obs['state']['player_info']['player_total_balance']))\n",
    "        print(\"Round over\")\n",
    "        print(\"-%-\"*30)\n",
    "        if (_round + 1) == (number_of_rounds):\n",
    "            print(\"Max number of rounds played. If you see this message, you are the winner.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RLAgent for Blackjack\n",
    "\n",
    "Referencing the RLAgent in the lecture note, make your own agent class for the game. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Explanation\n",
    "-  init Initializes the class and it's variables with the parameters\n",
    "-  init decay: Displays the decay amount\n",
    "-  update_param: Updates parameters based on inputs\n",
    "- train: Code to train the setup based on inputs, and previously stored Q and Reward tables.\n",
    "- test: Tests the trained model for a given input\n",
    "- show_strategy: Plots the data and shows the strategy and how it has been progressing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choice of TD Learning\n",
    "\n",
    "We have chosen to pick Q-Learning. The reason for this is :\n",
    "\n",
    "On-policy SARSA learns action values relative to the policy it follows, while off-policy Q-Learning does it relative to the greedy policy. Under some common conditions, they both converge to the real value function, but at different rates. Q-Learning tends to converge a little slower, but has the capabilitiy to continue learning while changing policies. Also, Q-Learning is not guaranteed to converge when combined with linear approximation.\n",
    "\n",
    "Q-learning's policy changes in long term can turn out to give a better convergence when compared with SARSA , and blackjack being a game with a good amount of learning curve, it might be beneficial to choose Q-Learning over SARSA.\n",
    "\n",
    "Q-learning directly learns the optimal policy, whilst SARSA learns a near-optimal policy whilst exploring. If you want to learn an optimal policy using SARSA, then you will need to decide on a strategy to decay ϵ in ϵ-greedy action choice, which may become a fiddly hyperparameter to tune.This will be ideal in the situation like blackjack, where the most optimal policy is ideal when we are iterating and drawing step by step. Hence Q-Learning was chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHOICE OF FUNCTION APPROXIMATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to choose different features for different actions given each step of the reinforcement learning algorithm. We need to focus on the key aspect of it , which is function approximation. Function approximation is the list of features which are used in what way to do the prediction in our given problem. \n",
    "\n",
    "<br>\n",
    "In our case, we notice that the total of the cards we have are the ideal representation of the states we are in. We use these parameters linearly to approximate the ideal action. We use MaxQ, from the actions at the given point when compared to the actions in the future state of the given next possible states. This is done using the max Q reinforcement learning, and the process of doing this may be the best way to achieve convergence. <br>\n",
    "\n",
    "- Approximating with exploited information would be ideal, especially in blackjack where the rules are the pre-bound information we are setting.\n",
    "- We choose max-Q , since it is the way we are combining the possible state actions with respect to the future state-actions. \n",
    "- At a given point we evaluate the options to us and pick the best possible approximation for the same in a linear manner.\n",
    "- We believe this to be the best way to deal with the problem at hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For the major part of our approximation, we use epsilon decay algorithm.\n",
    "- Learning rate is how big is taken a leap in finding optimal policy. In the terms of simple QLearning it's how much you are updating the Q value with each step. Higher alpha means you are updating your Q values in big steps. When the agent is learning you should decay this to stabilize your model output which eventually converges to an optimal policy.\n",
    "- Epsilon is used when we are selecting specific actions base on the Q values we already have. As an example if we select pure greedy method ( epsilon = 0 ) then we are always selecting the highest q value among the all the q values for a specific state. This causes issue in exploration as we can get stuck easily at a local optima.\n",
    "- Therefore , we introduce a greedy algorithm for the final steps to arrive at the solution/convergence. This is the goal we are expecting to see, and hence for the major part, we use epsilon decay to arrive closer to the optimum.\n",
    "- In conclusion learning rate is associated with how big you take a leap and epsilon is associated with how random you take an action. As the learning goes on both should decayed to stabilize and exploit the learned policy which converges to an optimal one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    " \n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, env, epsilon=1.0, alpha=0.5, gamma=0.9, num_episodes_to_train=30000, player_name='Ani'):\n",
    "        self.env = env\n",
    "        self.valid_actions = list(range(self.env.action_space.n))\n",
    "       # Set parameters of the learning agent\n",
    "        self.Q = dict()      # Q-table which will be a dictionary of tuples\n",
    "        self.epsilon = 0     # Random exploration factor\n",
    "        self.alpha = 0       # Learning factor\n",
    "        self.gamma = 0       # Discount factor- closer to 1 learns well into distant future\n",
    "        self.num_episodes_to_train = 0\n",
    "        self.player = Player(player_name)\n",
    "   \n",
    "\n",
    "    \n",
    "\n",
    "    def getNumeric(self, round_obs):\n",
    "        return (round_obs['player_info']['player_sum_card'], round_obs['player_info']['player_total_balance'],\n",
    "                round_obs['player_info']['usable_ace'])\n",
    "\n",
    "    \n",
    "\n",
    "    def init_decay(self,num_episodes_to_train):\n",
    "        self.num_episodes_to_train = num_episodes_to_train\n",
    "        self.small_decrement = (0.1 * self.epsilon) / (0.3 * self.num_episodes_to_train)\n",
    "        self.big_decrement = (0.8 * self.epsilon) / (0.4 * self.num_episodes_to_train)\n",
    "        self.num_episodes_to_train_left = self.num_episodes_to_train\n",
    "\n",
    " \n",
    "\n",
    "    def update_parameters(self):\n",
    "        if self.num_episodes_to_train_left > 0.7 * self.num_episodes_to_train:\n",
    "            self.epsilon -= self.small_decrement\n",
    "        elif self.num_episodes_to_train_left > 0.3 * self.num_episodes_to_train:\n",
    "            self.epsilon -= self.big_decrement\n",
    "        elif self.num_episodes_to_train_left > 0:\n",
    "            self.epsilon -= self.small_decrement\n",
    "        else:\n",
    "            self.epsilon = 0.0\n",
    "            self.alpha = 0.0\n",
    "        self.num_episodes_to_train_left -= 1\n",
    "\n",
    " \n",
    "\n",
    "    def create_Q_if_new_observation(self, observation):\n",
    "        if observation not in self.Q:\n",
    "            if observation == None:\n",
    "                pass\n",
    "            else:\n",
    "                print('---------------------')\n",
    "                print(observation)\n",
    "                print('---------------------')\n",
    "                self.Q[observation] = dict((action, 0.0) for action in self.valid_actions)\n",
    " \n",
    "    def get_maxQ(self, observation):\n",
    "        self.create_Q_if_new_observation(observation)\n",
    "        return max(self.Q[observation].values())\n",
    "   \n",
    "    def choose_action(self, observation):\n",
    "        self.create_Q_if_new_observation(observation)\n",
    "        if random.random() > self.epsilon:\n",
    "            maxQ = self.get_maxQ(observation)\n",
    "            action = random.choice([k for k in self.Q[observation].keys()\n",
    "                                    if self.Q[observation][k] == maxQ])\n",
    "        else:\n",
    "            action = random.choice(self.valid_actions)\n",
    "\n",
    "        self.update_parameters()\n",
    "        return action\n",
    "\n",
    "   \n",
    "\n",
    "    def get_action(self, observation):\n",
    "        self.create_Q_if_new_observation(observation)\n",
    "        maxQ = self.get_maxQ(observation)\n",
    "        return random.choice([k for k in self.Q[observation].keys()\n",
    "                                if self.Q[observation][k] == maxQ])\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "    def learn(self, observation, action, reward, next_observation):\n",
    "        if observation != None:\n",
    "            self.Q[observation][action] += self.alpha * (reward\n",
    "                                                     + (self.gamma * self.get_maxQ(next_observation))\n",
    "                                                     - self.Q[observation][action])\n",
    "        \n",
    "    def train(self, **params):\n",
    "        # parameters\n",
    "        self.gamma = params.pop('gamma', 0.2)\n",
    "        self.alpha = params.pop('alpha', 0.5)\n",
    "        self.epsilon= params.pop('epsilon', 1.0)\n",
    "        num_samples= params.pop('num_samples', 1000)\n",
    "        num_rounds= params.pop('num_rounds', 1000)\n",
    "        num_episodes_to_train = params.pop('num_episodes_to_train',800)\n",
    "        self.init_decay(num_episodes_to_train)\n",
    "        average_payouts = []\n",
    "        for _game in range(num_samples):\n",
    "            self.env.reset()\n",
    "            print(\"-\"* 50)\n",
    "            print(\"Game Number: {}\".format(_game + 1))\n",
    "            print(\"-\"* 50)\n",
    "            total_payout = 0\n",
    "            for _round in range(num_rounds):\n",
    "                print(\"%\"* 50)\n",
    "                print(\"Game Number: {} Round Number: {}\".format(_game+1, _round+1))\n",
    "                print(\"%\"* 50)\n",
    "                #Check if you have enough money left in the account to bet, if not break and start a new game\n",
    "                valid_bet_flag = self.env.get_valid_bet_amount(self.player.player_name)\n",
    "                if valid_bet_flag['is_round_done']:\n",
    "                    print(\"You are out of money ! Will go to next game !!\")\n",
    "                    break\n",
    "                bet_amount = valid_bet_flag['valid_bet_amount']\n",
    "                #Get the observations i.e state and use it to decide how much you want to bet\n",
    "                round_obs = self.env.init_round(self.player.player_name)\n",
    "                observation = self.getNumeric(round_obs['state'])\n",
    "                print(\"Initial state: {}\".format(round_obs)) \n",
    "                # If you have just $1 left in the bank, you have no choice but to bet that amount\n",
    "                # and hopefully leave it to the fate for your agent to win and continue playing or just\n",
    "                # start a new game\n",
    "                if len(bet_amount) == 1 and bet_amount[0] == 1:\n",
    "                    print(\"You can only bet 1. So betting only 1.\")\n",
    "                    random_bet = 1\n",
    "                else:\n",
    "                    random_bet = np.random.choice(bet_amount)\n",
    "#                     np.random.choice(bet_amount)\n",
    "                print(\"Random Init Bet: {}\".format(random_bet))\n",
    "                self.env.bet_money(self.player.player_name, random_bet)\n",
    "                # Look at the state and take actions, here the actions are selected randomly\n",
    "                # you can use the agent to select the action\n",
    "                random_action = self.choose_action(observation)\n",
    "                print(\"Action taken: {}\".format(random_action))\n",
    "                #Use the selected action to actually take the action in env by calling step\n",
    "                round_obs = self.env.step(self.player.player_name, random_action)\n",
    "                next_observation = self.getNumeric(round_obs['state'])\n",
    "                print(\"State after 1st action: {}\".format(round_obs))\n",
    "                self.learn(observation, random_action, round_obs['reward'], next_observation)\n",
    "                total_payout += round_obs['reward']\n",
    "                observation = self.getNumeric(round_obs['state'])\n",
    "                #Check if the game is not over, if your action is stand then the game is over, else\n",
    "                #the game continues in this loop until your action is stand\n",
    "                while not round_obs['is_round_done']:\n",
    "                    #Select an random action and take the action\n",
    "                    random_action = self.choose_action(observation)\n",
    "                    print(\"Action taken: {}\".format(random_action))\n",
    "                    round_obs = self.env.step(self.player.player_name, random_action)\n",
    "                    print(\"State after nth action: {}\".format(round_obs))\n",
    "                #Round over, but the game will continue untill you have exhausted your initial money\n",
    "                print(\"Balance Left: {}\".format(round_obs['state']['player_info']['player_total_balance']))\n",
    "                print(\"Round over\")\n",
    "                print(\"-%-\"*30)\n",
    "                if (_round + 1) == (number_of_rounds):\n",
    "                    print(\"Max number of rounds played. If you see this message, you are the winner.\")\n",
    "                average_payouts.append(total_payout)\n",
    "        return average_payouts\n",
    "\n",
    " \n",
    "\n",
    "   \n",
    "\n",
    "    def test(self,num_rounds):\n",
    "        agent_reward = []\n",
    "        agent_win = 0\n",
    "        agent_loss = 0\n",
    "        agent_draw = 0\n",
    "        for i in range(num_rounds+1):\n",
    "            self.env.reset()\n",
    "            # Start The Round and Start Observation\n",
    "            self.env.bet_money(self.player.player_name, random_bet)\n",
    "            test_round_obs = self.env.init_round(self.player.player_name)\n",
    "            obs = self.getNumeric(test_round_obs['state'])\n",
    "            # Init Payouts\n",
    "            total_payout = 0\n",
    "            is_round_done = False\n",
    "            while not is_round_done:\n",
    "                action = self.get_action(obs)\n",
    "                get_obs = self.env.step(self.player.player_name, action)\n",
    "                test_nxt_obs = self.getNumeric(get_obs['state'])\n",
    "                total_payout += get_obs['reward']\n",
    "                is_round_done = get_obs['is_round_done']\n",
    "                test_round_obs = test_nxt_obs\n",
    "            if(total_payout >0):\n",
    "                agent_win += 1\n",
    "            elif(total_payout < 0):\n",
    "                agent_loss += 1\n",
    "            else:\n",
    "                agent_draw += 1\n",
    "            agent_reward.append(total_payout)\n",
    "        winPect = agent_win / num_rounds * 100\n",
    "        lossPect = agent_loss / num_rounds * 100\n",
    "        drawPect = agent_draw / num_rounds * 100\n",
    "        return agent_reward , winPect, lossPect, drawPect\n",
    "\n",
    "    \n",
    "\n",
    "    def show_strategy(self):\n",
    "        list_players_hand = range(1, 22)\n",
    "        list_dealers_upcard = range(1, 11)\n",
    "        # Print headers to give more information about output\n",
    "        print (\"{:^10} | {:^51} | {:^51}\".format(\"Player's\",\"Dealer's upcard when ace is not usable\", \"Dealer's upcard when ace is usable\"))\n",
    "        print (\"{0:^10} | {1} | {1}\".format(\"Hand\", [str(upcard) if not upcard==1 else 'A'\n",
    "                                                        for upcard in list_dealers_upcard]))\n",
    "        for players_hand in list_players_hand:\n",
    "            actions_usable = []\n",
    "            actions_not_usable = []\n",
    "            for dealers_upcard in list_dealers_upcard:\n",
    "                observation = (players_hand, dealers_upcard, False)\n",
    "                actions_not_usable.append(self.readable_action(observation))\n",
    "                observation = (players_hand, dealers_upcard, True)\n",
    "                actions_usable.append(self.readable_action(observation))\n",
    "            print (\"{:>10} | {}  | {}\".format(players_hand, actions_not_usable, actions_usable))\n",
    "\n",
    " \n",
    "\n",
    "    def readable_action(self,observation):\n",
    "        \"\"\"\n",
    "        Pass observation to agent and get human readable action\n",
    "        H is hit, S is stick and '-' means the state is unseen and a random action is taken\n",
    "        \"\"\"\n",
    "        if observation not in self.Q:\n",
    "            action = \"-\"\n",
    "        else:\n",
    "            action = \"H\" if self.get_action(observation) else \"S\"   \n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result_blackJack(average_payouts,agent):\n",
    "    plot_avg_reward(average_payouts)\n",
    "    try_plot(agent)\n",
    "\n",
    "def try_plot(agent):\n",
    "    agent.show_strategy()\n",
    "    \n",
    "def summary(diff,wP, lP, dP):\n",
    "    print(\"Your Winning percentage is \", wP)\n",
    "    print(\"Your Losing percentage is \", lP)\n",
    "    print(\"The Percentage of game being a Draw is \", dP)\n",
    "    print(\"You have lost \",diff*-1,\" more games than the games you Won\")\n",
    "    \n",
    "def plot_avg_reward(average_payouts):\n",
    "    plt.plot(average_payouts)           \n",
    "    plt.xlabel('num_samples')\n",
    "    plt.ylabel('payout after 1000 rounds')\n",
    "    plt.show()      \n",
    "    print (\"Average payout after {} rounds is {}\".format(num_rounds, sum(average_payouts)/(num_samples)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alpha 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envb = BlackjackEnv()\n",
    "sam = Player('Ani')\n",
    "envb.add_player(sam)\n",
    "agent = Agent(envb)\n",
    "num_rounds = 100\n",
    "num_samples = 100\n",
    "num_episodes_to_train = 0.8* num_rounds\n",
    "# epsilon=1.0, alpha=0.5, gamma=0.2\n",
    "average_payouts = agent.train(epsilon=1.0, alpha=0.1, gamma=0.2,num_rounds = num_rounds,num_samples = num_samples,num_episodes_to_train=num_episodes_to_train)\n",
    "#plot_result_blackJack(average_payouts,agent)\n",
    "agent.show_strategy()\n",
    "# Test using Learned values\n",
    "rew,wP,lP,dP = agent.test(1000)\n",
    "#summary(np.sum(rew) , wP, lP, dP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_result_blackJack(average_payouts,agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alpha 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envb = BlackjackEnv()\n",
    "sam = Player('Ani')\n",
    "envb.add_player(sam)\n",
    "agent = Agent(envb)\n",
    "num_rounds = 100\n",
    "num_samples = 100\n",
    "num_episodes_to_train = 0.8* num_rounds\n",
    "# epsilon=1.0, alpha=0.5, gamma=0.2\n",
    "average_payouts = agent.train(epsilon=1.0, alpha=0.8, gamma=0.2,num_rounds = num_rounds,num_samples = num_samples,num_episodes_to_train=num_episodes_to_train)\n",
    "#plot_result_blackJack(average_payouts,agent)\n",
    "agent.show_strategy()\n",
    "# Test using Learned values\n",
    "rew,wP,lP,dP = agent.test(1000)\n",
    "#summary(np.sum(rew) , wP, lP, dP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_result_blackJack(average_payouts,agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alpha 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envb = BlackjackEnv()\n",
    "sam = Player('Ani')\n",
    "envb.add_player(sam)\n",
    "agent = Agent(envb)\n",
    "num_rounds = 100\n",
    "num_samples = 100\n",
    "num_episodes_to_train = 0.8* num_rounds\n",
    "# epsilon=1.0, alpha=0.5, gamma=0.2\n",
    "average_payouts = agent.train(epsilon=1.0, alpha=0.5, gamma=0.2,num_rounds = num_rounds,num_samples = num_samples,num_episodes_to_train=num_episodes_to_train)\n",
    "#plot_result_blackJack(average_payouts,agent)\n",
    "agent.show_strategy()\n",
    "# Test using Learned values\n",
    "rew,wP,lP,dP = agent.test(1000)\n",
    "#summary(np.sum(rew) , wP, lP, dP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_result_blackJack(average_payouts,agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gamma 0.6, Alpha 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envb = BlackjackEnv()\n",
    "sam = Player('Ani')\n",
    "envb.add_player(sam)\n",
    "agent = Agent(envb)\n",
    "num_rounds = 100\n",
    "num_samples = 100\n",
    "num_episodes_to_train = 0.8* num_rounds\n",
    "# epsilon=1.0, alpha=0.5, gamma=0.2\n",
    "average_payouts = agent.train(epsilon=1.0, alpha=0.5, gamma=0.2,num_rounds = num_rounds,num_samples = num_samples,num_episodes_to_train=num_episodes_to_train)\n",
    "#plot_result_blackJack(average_payouts,agent)\n",
    "agent.show_strategy()\n",
    "# Test using Learned values\n",
    "rew,wP,lP,dP = agent.test(1000)\n",
    "#summary(np.sum(rew) , wP, lP, dP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_result_blackJack(average_payouts,agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gamma 0.3, Alpha 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envb = BlackjackEnv()\n",
    "sam = Player('Ani')\n",
    "envb.add_player(sam)\n",
    "agent = Agent(envb)\n",
    "num_rounds = 100\n",
    "num_samples = 100\n",
    "num_episodes_to_train = 0.8* num_rounds\n",
    "# epsilon=1.0, alpha=0.5, gamma=0.2\n",
    "average_payouts = agent.train(epsilon=1.0, alpha=0.8, gamma=0.3,num_rounds = num_rounds,num_samples = num_samples,num_episodes_to_train=num_episodes_to_train)\n",
    "#plot_result_blackJack(average_payouts,agent)\n",
    "agent.show_strategy()\n",
    "# Test using Learned values\n",
    "rew,wP,lP,dP = agent.test(1000)\n",
    "#summary(np.sum(rew) , wP, lP, dP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_result_blackJack(average_payouts,agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection established, Names collected"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import os\n",
    "import subprocess\n",
    "import ast\n",
    "import numpy as np\n",
    "player = 'Anirudh' #enter unique name for your game\n",
    "s = socket.socket()\n",
    "host = '10.216.25.34' # change ip addres to ip adress of your computer or use 'localhost' to practice\n",
    "port = 9999\n",
    "\n",
    "s.connect((host, port))\n",
    "\n",
    "while True:\n",
    "    data = s.recv(1024)\n",
    "    if data.decode(\"utf-8\") == 'send':\n",
    "        s.send(str.encode( player))\n",
    "        client_response = str(s.recv(20480), \"utf-8\")\n",
    "        print(client_response, end=\"\")\n",
    "        break\n",
    "while True:\n",
    "    data = s.recv(1024)\n",
    "    if data.decode(\"utf-8\") == 'sendbet':\n",
    "        ro=str(s.recv(20480), \"utf-8\")\n",
    "        ro=ast.literal_eval(ro)\n",
    "        print(ro)\n",
    "# The above observation are stored in dict format. To access specific variables use syntax as \n",
    "# ro['state']['player_info']['player_total_balance']\n",
    "# send whichever variable information stored in ro to your agent to help make the decision\n",
    "#### Look at the above observations stored in variable ro as dictionary and store bet amount in variable bet#####\n",
    "        bet=2\n",
    "        s.send(str.encode( str(bet)))\n",
    "        \n",
    "    if data.decode(\"utf-8\") == 'sendaction':\n",
    "        ro=str(s.recv(20480), \"utf-8\")\n",
    "        ro=ast.literal_eval(ro)\n",
    "        print(ro)\n",
    "        #follow same procedure as abov to send observations to your agent\n",
    "        ###### Look at the above observations and action in variable action#####\n",
    "        \n",
    "        #ro = agent.getNumeric(ro)\n",
    "        action = np.random.randint(0, 2)\n",
    "        #action  = agent.get_action(ro)\n",
    "        \n",
    "        print(\"action \",action)\n",
    "        s.send(str.encode( str(action)))\n",
    "    if data.decode(\"utf-8\") == 'gameover':\n",
    "        print(\"Game over wait for others to play and wait for results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choice of parameters and Explanation of Plots\n",
    "\n",
    "The key goal of this assignment was to vary the so called hyperparameters. The hyperparameters in this case are Alpha , Gamme and epsilon, each of this we vary and set until we find a good parameter fit. \n",
    "\n",
    "We pick the following values of alpha <br>\n",
    "\n",
    "- 0.1\n",
    "- 0.8\n",
    "- 0.5\n",
    "- 0.1\n",
    "- 0.6\n",
    "\n",
    "We vary the gamma halfway into this variation by making it different values like 0.8, 0.6 0.3 and use epsilon 0.1. The core idea of this is to find the optimum parameter for convergence each of which have been described above. We notice a a good convergence with the following three parameters:\n",
    "<br>\n",
    "epsilon=1.0, alpha=0.8, gamma=0.3\n",
    "<br>\n",
    " The above parameters provide a decent convergence and a good learning rate. The way this works is that these optimum values lead to a good convergence with which we notice the right way to train our model so as to receive a good winning accuracy. <br>\n",
    " \n",
    " The plot for this seems accurate in this case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "We notice that for all the different methods, of reinforcement learning we found Q-Learning most suitable for our case, and we notice that a quick learning of the perfect policies may be most beneficial. We also notice that we selected a suitable plot with different values of alpha,gamma and epsilon selected. This will suite our acse the most. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignmentvenv",
   "language": "python",
   "name": "envname"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
